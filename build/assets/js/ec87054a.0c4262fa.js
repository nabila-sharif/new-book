"use strict";(self.webpackChunkfrontend_book=self.webpackChunkfrontend_book||[]).push([[7604],{8453(e,n,t){t.d(n,{R:()=>s,x:()=>r});var a=t(6540);const o={},i=a.createContext(o);function s(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),a.createElement(i.Provider,{value:n},e.children)}},9640(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>s,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"module-4/chapter-3-content","title":"Chapter 3: Capstone Project - Autonomous Humanoid","description":"Chapter 3: Capstone Project - Autonomous Humanoid","source":"@site/docs/module-4/chapter-3-content.md","sourceDirName":"module-4","slug":"/module-4/chapter-3-content","permalink":"/docs/module-4/chapter-3-content","draft":false,"unlisted":false,"editUrl":"https://github.com/nabila-sharif/AI---Humanoid-Robotics-Book/tree/main/frontend_book/docs/module-4/chapter-3-content.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Chapter 3: Capstone Project - Autonomous Humanoid"},"sidebar":"module4Sidebar","previous":{"title":"Chapter 2: Cognitive Planning with LLMs","permalink":"/docs/module-4/chapter-2-content"},"next":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/docs/module-4/"}}');var o=t(4848),i=t(8453);const s={sidebar_position:3,title:"Chapter 3: Capstone Project - Autonomous Humanoid"},r=void 0,c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Key Topics",id:"key-topics",level:2},{value:"1. End-to-End VLA Pipeline in Simulation",id:"1-end-to-end-vla-pipeline-in-simulation",level:3},{value:"Complete VLA System Architecture",id:"complete-vla-system-architecture",level:4},{value:"2. Navigation with Obstacle Avoidance",id:"2-navigation-with-obstacle-avoidance",level:3},{value:"Advanced Navigation System",id:"advanced-navigation-system",level:4},{value:"3. Object Detection and Manipulation",id:"3-object-detection-and-manipulation",level:3},{value:"Vision-Based Manipulation System",id:"vision-based-manipulation-system",level:4},{value:"4. Complete Demo Workflow: Voice \u2192 Plan \u2192 Act \u2192 Feedback",id:"4-complete-demo-workflow-voice--plan--act--feedback",level:3},{value:"Integrated Demo System",id:"integrated-demo-system",level:4},{value:"5. ROS 2 Action Graph Integration",id:"5-ros-2-action-graph-integration",level:3},{value:"ROS 2 Action Integration",id:"ros-2-action-integration",level:4},{value:"6. Simulation Environment Setup",id:"6-simulation-environment-setup",level:3},{value:"Simulation Configuration",id:"simulation-configuration",level:4},{value:"7. Complete Capstone Implementation",id:"7-complete-capstone-implementation",level:3},{value:"Assessment Criteria",id:"assessment-criteria",level:2}];function p(e){const n={code:"code",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.p,{children:"Chapter 3: Capstone Project - Autonomous Humanoid"}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Build an end-to-end Vision-Language-Action (VLA) pipeline in simulation"}),"\n",(0,o.jsx)(n.li,{children:"Implement navigation with obstacle avoidance for humanoid robots"}),"\n",(0,o.jsx)(n.li,{children:"Integrate object detection and manipulation capabilities"}),"\n",(0,o.jsx)(n.li,{children:"Create a complete demo workflow: voice \u2192 plan \u2192 act \u2192 feedback"}),"\n",(0,o.jsx)(n.li,{children:"Design ROS 2 action graph integration for complex tasks"}),"\n",(0,o.jsx)(n.li,{children:"Set up and configure simulation environments for humanoid robotics"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"key-topics",children:"Key Topics"}),"\n",(0,o.jsx)(n.h3,{id:"1-end-to-end-vla-pipeline-in-simulation",children:"1. End-to-End VLA Pipeline in Simulation"}),"\n",(0,o.jsx)(n.p,{children:"Creating a complete Vision-Language-Action pipeline involves integrating all components from voice input to robotic action execution in a simulated environment."}),"\n",(0,o.jsx)(n.h4,{id:"complete-vla-system-architecture",children:"Complete VLA System Architecture"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rospy\nimport whisper\nimport openai\nimport json\nimport numpy as np\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom move_base_msgs.msg import MoveBaseActionGoal\nfrom actionlib_msgs.msg import GoalStatusArray\nimport cv2\nfrom cv_bridge import CvBridge\n\nclass VLAPipeline:\n    def __init__(self):\n        rospy.init_node(\'vla_pipeline\')\n\n        # Initialize components\n        self.whisper_model = whisper.load_model("base")\n        openai.api_key = rospy.get_param(\'~openai_api_key\', \'your-key-here\')\n        self.cv_bridge = CvBridge()\n\n        # Publishers and subscribers\n        self.voice_sub = rospy.Subscriber(\'/voice_input\', String, self.voice_callback)\n        self.image_sub = rospy.Subscriber(\'/camera/rgb/image_raw\', Image, self.image_callback)\n        self.laser_sub = rospy.Subscriber(\'/scan\', LaserScan, self.laser_callback)\n\n        self.voice_cmd_pub = rospy.Publisher(\'/processed_voice_commands\', String, queue_size=10)\n        self.navigation_pub = rospy.Publisher(\'/move_base/goal\', MoveBaseActionGoal, queue_size=10)\n        self.cmd_vel_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\n        self.status_pub = rospy.Publisher(\'/vla_status\', String, queue_size=10)\n\n        # State variables\n        self.current_image = None\n        self.laser_data = None\n        self.robot_pose = None\n\n        rospy.loginfo("VLA Pipeline initialized")\n\n    def voice_callback(self, msg):\n        """Process voice commands through the full VLA pipeline"""\n        command = msg.data\n        rospy.loginfo(f"Received voice command: {command}")\n\n        # Update status\n        status_msg = String()\n        status_msg.data = f"Processing voice command: {command}"\n        self.status_pub.publish(status_msg)\n\n        # Plan based on voice command and current state\n        plan = self.generate_plan_from_voice_and_state(command)\n\n        # Execute the plan\n        if plan:\n            self.execute_plan(plan)\n\n    def image_callback(self, msg):\n        """Process camera images for vision component"""\n        try:\n            self.current_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\n        except Exception as e:\n            rospy.logerr(f"Error converting image: {e}")\n\n    def laser_callback(self, msg):\n        """Process laser scan data for navigation"""\n        self.laser_data = msg\n\n    def generate_plan_from_voice_and_state(self, voice_command):\n        """Generate a plan using LLM based on voice command and current state"""\n        # Get current state information\n        state_info = {\n            "current_image_available": self.current_image is not None,\n            "laser_data_available": self.laser_data is not None,\n            "environment_objects": self.detect_objects_in_image() if self.current_image is not None else []\n        }\n\n        # Create prompt for LLM\n        prompt = f"""\n        You are a cognitive planner for a humanoid robot. The robot has received the voice command: "{voice_command}"\n\n        Current state information:\n        - Objects detected: {state_info[\'environment_objects\']}\n        - Camera data available: {state_info[\'current_image_available\']}\n        - Laser data available: {state_info[\'laser_data_available\']}\n\n        Generate a sequence of actions to fulfill the command. The available actions are:\n        - move_to_location: Move to a specific location (x, y coordinates)\n        - detect_object: Look for a specific object in the environment\n        - approach_object: Move close to an object\n        - grasp_object: Pick up an object\n        - place_object: Place an object at a location\n        - speak: Make the robot speak a message\n\n        Return the plan as a JSON list of actions with parameters.\n        """\n\n        try:\n            response = openai.ChatCompletion.create(\n                model="gpt-4",\n                messages=[{"role": "user", "content": prompt}],\n                temperature=0.1\n            )\n\n            plan_text = response.choices[0].message.content\n            # Extract JSON from response\n            start_idx = plan_text.find(\'[\')\n            end_idx = plan_text.rfind(\']\') + 1\n            plan_json = plan_text[start_idx:end_idx]\n            plan = json.loads(plan_json)\n            return plan\n        except Exception as e:\n            rospy.logerr(f"Error generating plan: {e}")\n            return []\n\n    def detect_objects_in_image(self):\n        """Simple object detection in the current image"""\n        if self.current_image is None:\n            return []\n\n        # This is a simplified example - in practice you\'d use a proper object detection model\n        # For this example, we\'ll just return some placeholder objects\n        # In a real implementation, you\'d use YOLO, Detectron2, or similar\n        height, width, _ = self.current_image.shape\n\n        # Simulate object detection results\n        objects = []\n        if np.random.random() > 0.5:  # Randomly detect objects for simulation\n            objects.append({\n                "name": "red_cup",\n                "confidence": 0.85,\n                "bbox": [int(width*0.4), int(height*0.4), int(width*0.6), int(height*0.6)]\n            })\n        if np.random.random() > 0.7:\n            objects.append({\n                "name": "book",\n                "confidence": 0.78,\n                "bbox": [int(width*0.2), int(height*0.3), int(width*0.4), int(height*0.5)]\n            })\n\n        return objects\n\n    def execute_plan(self, plan):\n        """Execute a plan step by step"""\n        rospy.loginfo(f"Executing plan with {len(plan)} steps")\n\n        for i, action in enumerate(plan):\n            rospy.loginfo(f"Executing step {i+1}/{len(plan)}: {action[\'action\']}")\n\n            success = self.execute_action(action)\n\n            if not success:\n                rospy.logerr(f"Action failed: {action}")\n                # You could implement recovery strategies here\n                break\n\n        rospy.loginfo("Plan execution completed")\n\n    def execute_action(self, action):\n        """Execute a single action"""\n        action_name = action.get(\'action\', \'\')\n        params = action.get(\'parameters\', {})\n\n        if action_name == \'move_to_location\':\n            return self.move_to_location(params)\n        elif action_name == \'detect_object\':\n            return self.detect_object(params)\n        elif action_name == \'approach_object\':\n            return self.approach_object(params)\n        elif action_name == \'grasp_object\':\n            return self.grasp_object(params)\n        elif action_name == \'place_object\':\n            return self.place_object(params)\n        elif action_name == \'speak\':\n            return self.speak(params)\n        else:\n            rospy.logwarn(f"Unknown action: {action_name}")\n            return False\n\n    def move_to_location(self, params):\n        """Move to a specific location"""\n        x = params.get(\'x\', 0.0)\n        y = params.get(\'y\', 0.0)\n        theta = params.get(\'theta\', 0.0)\n\n        goal = MoveBaseActionGoal()\n        goal.header.stamp = rospy.Time.now()\n        goal.header.frame_id = "map"\n\n        goal.goal.target_pose.header.frame_id = "map"\n        goal.goal.target_pose.pose.position.x = x\n        goal.goal.target_pose.pose.position.y = y\n        goal.goal.target_pose.pose.orientation.z = np.sin(theta / 2)\n        goal.goal.target_pose.pose.orientation.w = np.cos(theta / 2)\n\n        self.navigation_pub.publish(goal)\n        rospy.loginfo(f"Moving to location: ({x}, {y}, {theta})")\n        return True\n\n    def detect_object(self, params):\n        """Detect a specific object"""\n        object_name = params.get(\'object_name\', \'any\')\n        rospy.loginfo(f"Looking for object: {object_name}")\n\n        # In a real system, this would trigger object detection\n        # For simulation, we\'ll just return success\n        return True\n\n    def approach_object(self, params):\n        """Approach an object"""\n        object_name = params.get(\'object_name\', \'unknown\')\n        rospy.loginfo(f"Approaching object: {object_name}")\n\n        # In a real system, this would navigate to the object\n        # For simulation, we\'ll just return success\n        return True\n\n    def grasp_object(self, params):\n        """Grasp an object"""\n        object_name = params.get(\'object_name\', \'unknown\')\n        rospy.loginfo(f"Grasping object: {object_name}")\n\n        # In a real system, this would trigger the gripper\n        # For simulation, we\'ll just return success\n        return True\n\n    def place_object(self, params):\n        """Place an object"""\n        object_name = params.get(\'object_name\', \'unknown\')\n        rospy.loginfo(f"Placing object: {object_name}")\n\n        # In a real system, this would release the gripper\n        # For simulation, we\'ll just return success\n        return True\n\n    def speak(self, params):\n        """Make the robot speak"""\n        message = params.get(\'message\', \'Hello\')\n        rospy.loginfo(f"Robot says: {message}")\n\n        # In a real system, this would trigger text-to-speech\n        # For simulation, we\'ll just log the message\n        return True\n\n    def run(self):\n        """Run the VLA pipeline"""\n        rospy.spin()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"2-navigation-with-obstacle-avoidance",children:"2. Navigation with Obstacle Avoidance"}),"\n",(0,o.jsx)(n.p,{children:"Implementing safe navigation in dynamic environments:"}),"\n",(0,o.jsx)(n.h4,{id:"advanced-navigation-system",children:"Advanced Navigation System"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class AdvancedNavigationSystem:\n    def __init__(self):\n        self.laser_sub = rospy.Subscriber(\'/scan\', LaserScan, self.laser_callback)\n        self.odom_sub = rospy.Subscriber(\'/odom\', Odometry, self.odom_callback)\n        self.cmd_vel_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\n\n        self.laser_data = None\n        self.current_pose = None\n        self.path = []\n        self.current_waypoint = 0\n\n    def laser_callback(self, msg):\n        """Process laser scan data for obstacle detection"""\n        self.laser_data = msg\n\n    def odom_callback(self, msg):\n        """Process odometry data for current pose"""\n        self.current_pose = msg.pose.pose\n\n    def check_for_obstacles(self, direction=\'forward\', distance=1.0):\n        """Check for obstacles in a specific direction"""\n        if not self.laser_data:\n            return False\n\n        # Calculate angle range for the direction\n        if direction == \'forward\':\n            start_angle = -15  # degrees\n            end_angle = 15\n        elif direction == \'left\':\n            start_angle = 75\n            end_angle = 105\n        elif direction == \'right\':\n            start_angle = -105\n            end_angle = -75\n        else:\n            start_angle = -180\n            end_angle = 180\n\n        # Convert angles to laser indices\n        angle_min = self.laser_data.angle_min\n        angle_increment = self.laser_data.angle_increment\n\n        start_idx = int((np.radians(start_angle) - angle_min) / angle_increment)\n        end_idx = int((np.radians(end_angle) - angle_min) / angle_increment)\n\n        start_idx = max(0, start_idx)\n        end_idx = min(len(self.laser_data.ranges), end_idx)\n\n        # Check for obstacles within the distance\n        for i in range(start_idx, end_idx):\n            if self.laser_data.ranges[i] < distance and not np.isnan(self.laser_data.ranges[i]):\n                return True\n\n        return False\n\n    def navigate_with_obstacle_avoidance(self, goal_x, goal_y):\n        """Navigate to goal with obstacle avoidance"""\n        rate = rospy.Rate(10)  # 10 Hz\n\n        while not rospy.is_shutdown():\n            if not self.current_pose:\n                rate.sleep()\n                continue\n\n            # Calculate direction to goal\n            current_x = self.current_pose.position.x\n            current_y = self.current_pose.position.y\n\n            dx = goal_x - current_x\n            dy = goal_y - current_y\n            distance_to_goal = np.sqrt(dx**2 + dy**2)\n\n            # Check if we\'ve reached the goal\n            if distance_to_goal < 0.5:  # 0.5m tolerance\n                cmd = Twist()\n                self.cmd_vel_pub.publish(cmd)  # Stop\n                rospy.loginfo("Reached goal")\n                return True\n\n            # Check for obstacles in the path\n            if self.check_for_obstacles(\'forward\', 1.0):\n                # Implement obstacle avoidance behavior\n                cmd = Twist()\n                if self.check_for_obstacles(\'left\', 1.0):\n                    # Turn right if left is blocked\n                    cmd.angular.z = -0.5\n                else:\n                    # Turn left to avoid obstacle\n                    cmd.angular.z = 0.5\n                cmd.linear.x = 0.1  # Slow forward movement\n            else:\n                # Move toward goal\n                cmd = Twist()\n                cmd.linear.x = min(0.5, distance_to_goal)  # Scale speed with distance\n                cmd.angular.z = np.arctan2(dy, dx) - self.current_pose.orientation.z\n\n            self.cmd_vel_pub.publish(cmd)\n            rate.sleep()\n\n        return False\n'})}),"\n",(0,o.jsx)(n.h3,{id:"3-object-detection-and-manipulation",children:"3. Object Detection and Manipulation"}),"\n",(0,o.jsx)(n.p,{children:"Integrating vision and manipulation capabilities:"}),"\n",(0,o.jsx)(n.h4,{id:"vision-based-manipulation-system",children:"Vision-Based Manipulation System"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class VisionBasedManipulation:\n    def __init__(self):\n        self.image_sub = rospy.Subscriber(\'/camera/rgb/image_raw\', Image, self.image_callback)\n        self.depth_sub = rospy.Subscriber(\'/camera/depth/image_raw\', Image, self.depth_callback)\n        self.manipulation_pub = rospy.Publisher(\'/manipulation_commands\', String, queue_size=10)\n\n        self.cv_bridge = CvBridge()\n        self.current_image = None\n        self.current_depth = None\n        self.detected_objects = []\n\n    def image_callback(self, msg):\n        """Process camera image for object detection"""\n        try:\n            self.current_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\n            self.detected_objects = self.detect_objects(self.current_image)\n        except Exception as e:\n            rospy.logerr(f"Error processing image: {e}")\n\n    def depth_callback(self, msg):\n        """Process depth image for 3D information"""\n        try:\n            self.current_depth = self.cv_bridge.imgmsg_to_cv2(msg, "32FC1")\n        except Exception as e:\n            rospy.logerr(f"Error processing depth image: {e}")\n\n    def detect_objects(self, image):\n        """Detect objects in the image"""\n        # This is a simplified example using color-based detection\n        # In practice, you\'d use a deep learning model like YOLO\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Define color ranges for different objects\n        color_ranges = {\n            \'red_cup\': (np.array([0, 50, 50]), np.array([10, 255, 255])),\n            \'blue_bottle\': (np.array([100, 50, 50]), np.array([130, 255, 255])),\n            \'green_box\': (np.array([50, 50, 50]), np.array([70, 255, 255]))\n        }\n\n        detected = []\n        for obj_name, (lower, upper) in color_ranges.items():\n            mask = cv2.inRange(hsv, lower, upper)\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            for contour in contours:\n                area = cv2.contourArea(contour)\n                if area > 500:  # Filter out small detections\n                    x, y, w, h = cv2.boundingRect(contour)\n                    center_x, center_y = x + w//2, y + h//2\n\n                    # Get depth at center of object\n                    depth = self.get_depth_at_pixel(center_x, center_y) if self.current_depth is not None else None\n\n                    detected.append({\n                        \'name\': obj_name,\n                        \'bbox\': [x, y, x+w, y+h],\n                        \'center\': (center_x, center_y),\n                        \'depth\': depth,\n                        \'confidence\': 0.8  # Simplified confidence\n                    })\n\n        return detected\n\n    def get_depth_at_pixel(self, x, y):\n        """Get depth value at a specific pixel"""\n        if self.current_depth is not None and 0 <= x < self.current_depth.shape[1] and 0 <= y < self.current_depth.shape[0]:\n            return self.current_depth[y, x]\n        return None\n\n    def find_object_by_name(self, name):\n        """Find a specific object by name"""\n        for obj in self.detected_objects:\n            if obj[\'name\'] == name:\n                return obj\n        return None\n\n    def approach_object(self, obj_name):\n        """Approach a specific object"""\n        obj = self.find_object_by_name(obj_name)\n        if not obj:\n            rospy.logerr(f"Object {obj_name} not found")\n            return False\n\n        # Calculate approach position (in front of object)\n        if obj[\'depth\']:\n            approach_x = obj[\'center\'][0]\n            approach_y = obj[\'center\'][1]\n            distance = max(0.3, obj[\'depth\'] - 0.2)  # 20cm from object\n\n            # Send approach command\n            cmd = String()\n            cmd.data = f"approach_object:{obj_name}:{distance}"\n            self.manipulation_pub.publish(cmd)\n            return True\n        else:\n            rospy.logerr(f"No depth information for object {obj_name}")\n            return False\n\n    def grasp_object(self, obj_name):\n        """Grasp a specific object"""\n        obj = self.find_object_by_name(obj_name)\n        if not obj:\n            rospy.logerr(f"Object {obj_name} not found")\n            return False\n\n        # Send grasp command\n        cmd = String()\n        cmd.data = f"grasp_object:{obj_name}"\n        self.manipulation_pub.publish(cmd)\n        rospy.loginfo(f"Attempting to grasp {obj_name}")\n        return True\n'})}),"\n",(0,o.jsx)(n.h3,{id:"4-complete-demo-workflow-voice--plan--act--feedback",children:"4. Complete Demo Workflow: Voice \u2192 Plan \u2192 Act \u2192 Feedback"}),"\n",(0,o.jsx)(n.p,{children:"Creating a complete demonstration system:"}),"\n",(0,o.jsx)(n.h4,{id:"integrated-demo-system",children:"Integrated Demo System"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class IntegratedDemoSystem:\n    def __init__(self):\n        rospy.init_node(\'integrated_demo_system\')\n\n        # Initialize all components\n        self.vla_pipeline = VLAPipeline()\n        self.nav_system = AdvancedNavigationSystem()\n        self.vision_manip = VisionBasedManipulation()\n\n        # Demo state\n        self.demo_state = "idle"\n        self.demo_steps = []\n        self.current_step = 0\n\n    def start_demo(self, voice_command):\n        """Start the complete demo workflow"""\n        rospy.loginfo(f"Starting demo with command: {voice_command}")\n\n        # Step 1: Voice processing\n        self.demo_state = "voice_processing"\n        rospy.loginfo("Step 1: Processing voice command")\n\n        # Generate plan from voice command\n        plan = self.vla_pipeline.generate_plan_from_voice_and_state(voice_command)\n\n        if not plan:\n            rospy.logerr("Could not generate plan from voice command")\n            return False\n\n        rospy.loginfo(f"Generated plan with {len(plan)} steps")\n\n        # Step 2: Plan execution\n        self.demo_state = "planning"\n        rospy.loginfo("Step 2: Executing plan")\n\n        # Execute the plan\n        success = self.vla_pipeline.execute_plan(plan)\n\n        if success:\n            # Step 3: Action execution with feedback\n            self.demo_state = "acting"\n            rospy.loginfo("Step 3: Actions completed, providing feedback")\n\n            # Provide success feedback\n            feedback_msg = String()\n            feedback_msg.data = f"Successfully completed task: {voice_command}"\n            self.vla_pipeline.status_pub.publish(feedback_msg)\n\n            self.demo_state = "completed"\n            rospy.loginfo("Demo completed successfully")\n            return True\n        else:\n            rospy.logerr("Plan execution failed")\n            feedback_msg = String()\n            feedback_msg.data = f"Failed to complete task: {voice_command}"\n            self.vla_pipeline.status_pub.publish(feedback_msg)\n\n            self.demo_state = "failed"\n            return False\n\n    def run_predefined_demo(self):\n        """Run a predefined demo sequence"""\n        demo_commands = [\n            "Go to the kitchen and bring me a red cup",\n            "Navigate to the living room and find the book",\n            "Move to the bedroom and turn left"\n        ]\n\n        for i, command in enumerate(demo_commands):\n            rospy.loginfo(f"Running demo {i+1}/{len(demo_commands)}: {command}")\n\n            success = self.start_demo(command)\n\n            if not success:\n                rospy.logerr(f"Demo {i+1} failed")\n                break\n\n            # Wait between demos\n            rospy.sleep(5.0)\n\n        rospy.loginfo("All demos completed")\n\n    def run(self):\n        """Run the integrated demo system"""\n        rospy.loginfo("Integrated Demo System running")\n\n        # Example: Start a predefined demo\n        self.run_predefined_demo()\n\n        rospy.spin()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"5-ros-2-action-graph-integration",children:"5. ROS 2 Action Graph Integration"}),"\n",(0,o.jsx)(n.p,{children:"For ROS 2 systems, integrating action graphs:"}),"\n",(0,o.jsx)(n.h4,{id:"ros-2-action-integration",children:"ROS 2 Action Integration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.action import ActionClient\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\nfrom nav2_msgs.action import NavigateToPose\nfrom example_interfaces.action import Fibonacci\n\nclass ROS2ActionIntegrator(Node):\n    def __init__(self):\n        super().__init__('ros2_action_integrator')\n\n        # Create action clients\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n\n        # Wait for action servers\n        self.nav_client.wait_for_server()\n\n    async def navigate_to_pose_async(self, x, y, theta):\n        \"\"\"Asynchronously navigate to a pose\"\"\"\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = 'map'\n        goal_msg.pose.pose.position.x = x\n        goal_msg.pose.pose.position.y = y\n        goal_msg.pose.pose.orientation.z = np.sin(theta / 2)\n        goal_msg.pose.pose.orientation.w = np.cos(theta / 2)\n\n        self.get_logger().info(f'Waiting for action server to navigate to ({x}, {y}, {theta})')\n\n        goal_handle = await self.nav_client.send_goal_async(goal_msg)\n\n        if not goal_handle.accepted:\n            self.get_logger().info('Goal rejected')\n            return False\n\n        self.get_logger().info('Goal accepted')\n\n        result = await goal_handle.get_result_async()\n        status = result.result\n        self.get_logger().info(f'Result: {status}')\n\n        return True\n\n    def create_action_graph(self, plan):\n        \"\"\"Create an action graph from a plan\"\"\"\n        # This would create a directed graph of actions with dependencies\n        # For this example, we'll just return a sequence\n        action_graph = {\n            'nodes': [],\n            'edges': []\n        }\n\n        for i, action in enumerate(plan):\n            node = {\n                'id': i,\n                'action': action,\n                'dependencies': [] if i == 0 else [i-1]  # Sequential dependencies\n            }\n            action_graph['nodes'].append(node)\n\n            if i > 0:\n                edge = {\n                    'from': i-1,\n                    'to': i,\n                    'type': 'sequential'\n                }\n                action_graph['edges'].append(edge)\n\n        return action_graph\n"})}),"\n",(0,o.jsx)(n.h3,{id:"6-simulation-environment-setup",children:"6. Simulation Environment Setup"}),"\n",(0,o.jsx)(n.p,{children:"Setting up the simulation environment:"}),"\n",(0,o.jsx)(n.h4,{id:"simulation-configuration",children:"Simulation Configuration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# simulation_setup.launch.py (for ROS 2)\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    # Declare launch arguments\n    world_arg = DeclareLaunchArgument(\n        'world',\n        default_value='small_room.world',\n        description='Choose one of the world files from `/gazebo_ros_pkgs/gazebo_worlds`'\n    )\n\n    # Launch Gazebo\n    gazebo = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                FindPackageShare('gazebo_ros'),\n                'launch',\n                'gazebo.launch.py'\n            ])\n        ]),\n        launch_arguments={\n            'world': LaunchConfiguration('world')\n        }.items()\n    )\n\n    # Launch robot model in Gazebo\n    spawn_entity = Node(\n        package='gazebo_ros',\n        executable='spawn_entity.py',\n        arguments=[\n            '-topic', 'robot_description',\n            '-entity', 'humanoid_robot',\n            '-x', '0.0',\n            '-y', '0.0',\n            '-z', '0.0'\n        ],\n        output='screen'\n    )\n\n    # Launch VLA pipeline\n    vla_pipeline = Node(\n        package='vla_package',\n        executable='vla_pipeline',\n        name='vla_pipeline',\n        parameters=[\n            {'openai_api_key': 'your-api-key-here'}\n        ],\n        output='screen'\n    )\n\n    # Launch navigation stack\n    navigation = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                FindPackageShare('nav2_bringup'),\n                'launch',\n                'navigation_launch.py'\n            ])\n        ])\n    )\n\n    return LaunchDescription([\n        world_arg,\n        gazebo,\n        spawn_entity,\n        vla_pipeline,\n        navigation\n    ])\n"})}),"\n",(0,o.jsx)(n.h3,{id:"7-complete-capstone-implementation",children:"7. Complete Capstone Implementation"}),"\n",(0,o.jsx)(n.p,{children:"Here's the complete capstone project implementation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rospy\nimport whisper\nimport openai\nimport json\nimport numpy as np\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom move_base_msgs.msg import MoveBaseActionGoal\nfrom cv_bridge import CvBridge\n\nclass AutonomousHumanoidCapstone:\n    def __init__(self):\n        rospy.init_node(\'autonomous_humanoid_capstone\')\n\n        # Initialize all components\n        self.whisper_model = whisper.load_model("base")\n        openai.api_key = rospy.get_param(\'~openai_api_key\', \'your-api-key-here\')\n        self.cv_bridge = CvBridge()\n\n        # Publishers and subscribers\n        self.voice_sub = rospy.Subscriber(\'/voice_input\', String, self.voice_command_callback)\n        self.image_sub = rospy.Subscriber(\'/camera/rgb/image_raw\', Image, self.image_callback)\n        self.laser_sub = rospy.Subscriber(\'/scan\', LaserScan, self.laser_callback)\n\n        self.voice_cmd_pub = rospy.Publisher(\'/processed_voice_commands\', String, queue_size=10)\n        self.navigation_pub = rospy.Publisher(\'/move_base/goal\', MoveBaseActionGoal, queue_size=10)\n        self.cmd_vel_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\n        self.status_pub = rospy.Publisher(\'/capstone_status\', String, queue_size=10)\n        self.manipulation_pub = rospy.Publisher(\'/manipulation_commands\', String, queue_size=10)\n\n        # State variables\n        self.current_image = None\n        self.laser_data = None\n        self.system_state = "idle"\n        self.detected_objects = []\n\n        rospy.loginfo("Autonomous Humanoid Capstone System initialized")\n\n    def voice_command_callback(self, msg):\n        """Main callback for processing voice commands"""\n        command = msg.data\n        rospy.loginfo(f"Received voice command: {command}")\n\n        # Update status\n        status_msg = String()\n        status_msg.data = f"Processing voice command: {command}"\n        self.status_pub.publish(status_msg)\n\n        # Execute the full VLA pipeline\n        success = self.execute_vla_pipeline(command)\n\n        if success:\n            status_msg.data = f"Successfully completed: {command}"\n        else:\n            status_msg.data = f"Failed to complete: {command}"\n\n        self.status_pub.publish(status_msg)\n\n    def image_callback(self, msg):\n        """Process camera images"""\n        try:\n            self.current_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\n            self.detected_objects = self.detect_objects_in_image()\n        except Exception as e:\n            rospy.logerr(f"Error processing image: {e}")\n\n    def laser_callback(self, msg):\n        """Process laser scan data"""\n        self.laser_data = msg\n\n    def detect_objects_in_image(self):\n        """Detect objects in the current image"""\n        if self.current_image is None:\n            return []\n\n        # Simplified object detection (in practice, use a trained model)\n        height, width, _ = self.current_image.shape\n\n        objects = []\n        # Simulate detecting some objects\n        if np.random.random() > 0.3:\n            objects.append({\n                "name": "red_cup",\n                "confidence": 0.85,\n                "bbox": [int(width*0.4), int(height*0.4), int(width*0.6), int(height*0.6)],\n                "center": (int(width*0.5), int(height*0.5))\n            })\n        if np.random.random() > 0.5:\n            objects.append({\n                "name": "book",\n                "confidence": 0.78,\n                "bbox": [int(width*0.2), int(height*0.3), int(width*0.4), int(height*0.5)],\n                "center": (int(width*0.3), int(height*0.4))\n            })\n\n        return objects\n\n    def generate_plan_with_llm(self, command, state_info):\n        """Generate a plan using LLM based on command and state"""\n        prompt = f"""\n        You are a cognitive planner for an autonomous humanoid robot.\n        The robot has received the command: "{command}"\n\n        Current state information:\n        - Objects detected: {[obj[\'name\'] for obj in state_info[\'detected_objects\']]}\n        - Camera data available: {state_info[\'camera_available\']}\n        - Laser data available: {state_info[\'laser_available\']}\n\n        Generate a detailed sequence of actions to fulfill the command.\n        Consider the detected objects and environmental constraints.\n\n        Available actions:\n        - navigate_to: Move to a location (x, y coordinates)\n        - detect_object: Look for a specific object\n        - approach_object: Move close to an object\n        - grasp_object: Pick up an object\n        - place_object: Place an object at a location\n        - speak: Make the robot speak a message\n        - wait: Pause for a specified time\n\n        Return the plan as a JSON list of actions with parameters.\n        Each action should have: action, parameters, and reasoning.\n        """\n\n        try:\n            response = openai.ChatCompletion.create(\n                model="gpt-4",\n                messages=[{"role": "user", "content": prompt}],\n                temperature=0.1\n            )\n\n            plan_text = response.choices[0].message.content\n            # Extract JSON from response\n            start_idx = plan_text.find(\'[\')\n            end_idx = plan_text.rfind(\']\') + 1\n            plan_json = plan_text[start_idx:end_idx]\n            plan = json.loads(plan_json)\n            return plan\n        except Exception as e:\n            rospy.logerr(f"Error generating plan with LLM: {e}")\n            return []\n\n    def execute_vla_pipeline(self, command):\n        """Execute the complete VLA pipeline: Voice \u2192 Language \u2192 Action"""\n        rospy.loginfo("Starting VLA pipeline execution")\n\n        # Gather current state\n        state_info = {\n            "detected_objects": self.detected_objects,\n            "camera_available": self.current_image is not None,\n            "laser_available": self.laser_data is not None\n        }\n\n        # Step 1: Plan generation using LLM\n        rospy.loginfo("Step 1: Generating plan with LLM")\n        plan = self.generate_plan_with_llm(command, state_info)\n\n        if not plan:\n            rospy.logerr("Could not generate plan")\n            return False\n\n        rospy.loginfo(f"Generated plan with {len(plan)} steps")\n\n        # Step 2: Plan execution\n        rospy.loginfo("Step 2: Executing plan")\n        success = self.execute_plan(plan)\n\n        if success:\n            rospy.loginfo("VLA pipeline completed successfully")\n            return True\n        else:\n            rospy.logerr("VLA pipeline execution failed")\n            return False\n\n    def execute_plan(self, plan):\n        """Execute a plan step by step"""\n        for i, action in enumerate(plan):\n            rospy.loginfo(f"Executing action {i+1}/{len(plan)}: {action.get(\'action\', \'unknown\')}")\n\n            success = self.execute_single_action(action)\n\n            if not success:\n                rospy.logerr(f"Action failed: {action}")\n                return False\n\n            # Small delay between actions\n            rospy.sleep(0.5)\n\n        return True\n\n    def execute_single_action(self, action):\n        """Execute a single action"""\n        action_name = action.get(\'action\', \'\')\n        params = action.get(\'parameters\', {})\n\n        if action_name == \'navigate_to\':\n            return self.navigate_to_location(params)\n        elif action_name == \'detect_object\':\n            return self.detect_object_action(params)\n        elif action_name == \'approach_object\':\n            return self.approach_object_action(params)\n        elif action_name == \'grasp_object\':\n            return self.grasp_object_action(params)\n        elif action_name == \'place_object\':\n            return self.place_object_action(params)\n        elif action_name == \'speak\':\n            return self.speak_action(params)\n        elif action_name == \'wait\':\n            return self.wait_action(params)\n        else:\n            rospy.logwarn(f"Unknown action: {action_name}")\n            return False\n\n    def navigate_to_location(self, params):\n        """Navigate to a specific location"""\n        x = params.get(\'x\', 0.0)\n        y = params.get(\'y\', 0.0)\n        theta = params.get(\'theta\', 0.0)\n\n        goal = MoveBaseActionGoal()\n        goal.header.stamp = rospy.Time.now()\n        goal.header.frame_id = "map"\n\n        goal.goal.target_pose.header.frame_id = "map"\n        goal.goal.target_pose.pose.position.x = x\n        goal.goal.target_pose.pose.position.y = y\n        goal.goal.target_pose.pose.orientation.z = np.sin(theta / 2)\n        goal.goal.target_pose.pose.orientation.w = np.cos(theta / 2)\n\n        self.navigation_pub.publish(goal)\n        rospy.loginfo(f"Navigating to ({x}, {y}, {theta})")\n        rospy.sleep(2.0)  # Simulate navigation time\n        return True\n\n    def detect_object_action(self, params):\n        """Detect a specific object"""\n        object_name = params.get(\'object_name\', \'any\')\n        rospy.loginfo(f"Detecting object: {object_name}")\n\n        # In a real system, this would trigger object detection\n        # For simulation, we\'ll just check if the object is in detected_objects\n        for obj in self.detected_objects:\n            if object_name.lower() in obj[\'name\'].lower():\n                rospy.loginfo(f"Found {obj[\'name\']}")\n                return True\n\n        rospy.loginfo(f"Object {object_name} not found in current view")\n        return True  # Not a failure, just not detected\n\n    def approach_object_action(self, params):\n        """Approach an object"""\n        object_name = params.get(\'object_name\', \'unknown\')\n        rospy.loginfo(f"Approaching object: {object_name}")\n\n        # In a real system, this would navigate to the object\n        # For simulation, we\'ll just return success\n        rospy.sleep(1.0)\n        return True\n\n    def grasp_object_action(self, params):\n        """Grasp an object"""\n        object_name = params.get(\'object_name\', \'unknown\')\n        rospy.loginfo(f"Grasping object: {object_name}")\n\n        # In a real system, this would trigger the gripper\n        # For simulation, we\'ll just return success\n        rospy.sleep(1.0)\n        return True\n\n    def place_object_action(self, params):\n        """Place an object"""\n        object_name = params.get(\'object_name\', \'unknown\')\n        rospy.loginfo(f"Placing object: {object_name}")\n\n        # In a real system, this would release the gripper\n        # For simulation, we\'ll just return success\n        rospy.sleep(1.0)\n        return True\n\n    def speak_action(self, params):\n        """Make the robot speak"""\n        message = params.get(\'message\', \'Hello\')\n        rospy.loginfo(f"Robot says: {message}")\n\n        # In a real system, this would trigger text-to-speech\n        # For simulation, we\'ll just log the message\n        return True\n\n    def wait_action(self, params):\n        """Wait for a specified time"""\n        duration = params.get(\'duration\', 1.0)\n        rospy.loginfo(f"Waiting for {duration} seconds")\n        rospy.sleep(duration)\n        return True\n\n    def run(self):\n        """Run the capstone system"""\n        rospy.loginfo("Autonomous Humanoid Capstone System running")\n        rospy.spin()\n\nif __name__ == \'__main__\':\n    capstone = AutonomousHumanoidCapstone()\n    capstone.run()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"assessment-criteria",children:"Assessment Criteria"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Students can build a complete end-to-end VLA pipeline in simulation"}),"\n",(0,o.jsx)(n.li,{children:"Students can implement navigation with obstacle avoidance for humanoid robots"}),"\n",(0,o.jsx)(n.li,{children:"Students can integrate object detection and manipulation capabilities"}),"\n",(0,o.jsx)(n.li,{children:"Students can create a complete demo workflow from voice to action with feedback"}),"\n",(0,o.jsx)(n.li,{children:"Students can design ROS 2 action graph integration for complex tasks"}),"\n",(0,o.jsx)(n.li,{children:"Students can set up and configure simulation environments for humanoid robotics"}),"\n",(0,o.jsx)(n.li,{children:"Students can demonstrate the complete autonomous humanoid system with voice commands"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}}}]);