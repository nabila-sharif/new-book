"use strict";(self.webpackChunkfrontend_book=self.webpackChunkfrontend_book||[]).push([[3267],{454(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-4/index","title":"Module 4: Vision-Language-Action (VLA)","description":"Module 4: Vision-Language-Action (VLA)","source":"@site/docs/module-4/index.md","sourceDirName":"module-4","slug":"/module-4/","permalink":"/docs/module-4/","draft":false,"unlisted":false,"editUrl":"https://github.com/nabila-sharif/AI---Humanoid-Robotics-Book/tree/main/frontend_book/docs/module-4/index.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Module 4: Vision-Language-Action (VLA)"},"sidebar":"module4Sidebar","next":{"title":"Chapter 1: Voice-to-Action Interfaces","permalink":"/docs/module-4/chapter-1-content"}}');var o=i(4848),s=i(8453);const a={sidebar_position:4,title:"Module 4: Vision-Language-Action (VLA)"},r=void 0,c={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Chapter Breakdown",id:"chapter-breakdown",level:2},{value:"Chapter 1: Voice-to-Action Interfaces",id:"chapter-1-voice-to-action-interfaces",level:3},{value:"Chapter 2: Cognitive Planning with LLMs",id:"chapter-2-cognitive-planning-with-llms",level:3},{value:"Chapter 3: Capstone Project - Autonomous Humanoid",id:"chapter-3-capstone-project---autonomous-humanoid",level:3},{value:"Prerequisites",id:"prerequisites",level:2}];function d(e){const n={h2:"h2",h3:"h3",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.p,{children:"Module 4: Vision-Language-Action (VLA)"}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"Welcome to Module 4 of Physical AI & Humanoid Robotics, focusing on Vision-Language-Action (VLA) systems. This module explores the convergence of Large Language Models (LLMs) and robotics, demonstrating how to create intelligent systems that can understand voice commands, plan actions cognitively, and execute robotic tasks in simulation."}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this module, you will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implement voice-to-action interfaces using OpenAI Whisper"}),"\n",(0,o.jsx)(n.li,{children:"Design cognitive planning systems with LLMs that translate natural language goals into ROS 2 action sequences"}),"\n",(0,o.jsx)(n.li,{children:"Build an end-to-end autonomous humanoid system that integrates vision, language, and action in simulation"}),"\n",(0,o.jsx)(n.li,{children:"Understand the architecture of VLA systems and their applications in robotics"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"chapter-breakdown",children:"Chapter Breakdown"}),"\n",(0,o.jsx)(n.h3,{id:"chapter-1-voice-to-action-interfaces",children:"Chapter 1: Voice-to-Action Interfaces"}),"\n",(0,o.jsx)(n.p,{children:"Learn how to integrate OpenAI Whisper for speech-to-text conversion, convert voice commands into structured task intents, and publish intents to ROS 2 topics/actions."}),"\n",(0,o.jsx)(n.h3,{id:"chapter-2-cognitive-planning-with-llms",children:"Chapter 2: Cognitive Planning with LLMs"}),"\n",(0,o.jsx)(n.p,{children:"Explore how to use LLMs for goal decomposition and task sequencing, translate natural language goals into ROS 2 action graphs, and implement error handling, replanning, and safety constraints."}),"\n",(0,o.jsx)(n.h3,{id:"chapter-3-capstone-project---autonomous-humanoid",children:"Chapter 3: Capstone Project - Autonomous Humanoid"}),"\n",(0,o.jsx)(n.p,{children:"Build a complete end-to-end VLA pipeline in simulation, including navigation with obstacle avoidance, object detection and manipulation, and a full demo from voice command to action feedback."}),"\n",(0,o.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsx)(n.p,{children:"Before starting this module, you should have:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Basic understanding of ROS 2 concepts"}),"\n",(0,o.jsx)(n.li,{children:"Familiarity with Python and robotics simulation"}),"\n",(0,o.jsx)(n.li,{children:"Knowledge of previous modules in this series"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>a,x:()=>r});var t=i(6540);const o={},s=t.createContext(o);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);