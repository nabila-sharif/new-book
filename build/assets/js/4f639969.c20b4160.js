"use strict";(self.webpackChunkfrontend_book=self.webpackChunkfrontend_book||[]).push([[6441],{284(n,e,i){i.r(e),i.d(e,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"module-4/chapter-1-content","title":"Chapter 1: Voice-to-Action Interfaces","description":"Chapter 1: Voice-to-Action Interfaces","source":"@site/docs/module-4/chapter-1-content.md","sourceDirName":"module-4","slug":"/module-4/chapter-1-content","permalink":"/docs/module-4/chapter-1-content","draft":false,"unlisted":false,"editUrl":"https://github.com/nabila-sharif/AI---Humanoid-Robotics-Book/tree/main/frontend_book/docs/module-4/chapter-1-content.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Chapter 1: Voice-to-Action Interfaces"},"sidebar":"module4Sidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/docs/module-4/"},"next":{"title":"Chapter 2: Cognitive Planning with LLMs","permalink":"/docs/module-4/chapter-2-content"}}');var t=i(4848),s=i(8453);const a={sidebar_position:1,title:"Chapter 1: Voice-to-Action Interfaces"},r=void 0,c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Key Topics",id:"key-topics",level:2},{value:"1. OpenAI Whisper Integration",id:"1-openai-whisper-integration",level:3},{value:"Installation and Setup",id:"installation-and-setup",level:4},{value:"Basic Whisper Implementation",id:"basic-whisper-implementation",level:4},{value:"2. Speech-to-Text Conversion Processes",id:"2-speech-to-text-conversion-processes",level:3},{value:"Audio Preprocessing Example",id:"audio-preprocessing-example",level:4},{value:"3. Voice Command Processing Pipelines",id:"3-voice-command-processing-pipelines",level:3},{value:"Pipeline Architecture",id:"pipeline-architecture",level:4},{value:"4. Structured Task Intents Mapping",id:"4-structured-task-intents-mapping",level:3},{value:"Intent Mapping Example",id:"intent-mapping-example",level:4},{value:"5. Practical Implementation: Voice-to-Action System",id:"5-practical-implementation-voice-to-action-system",level:3},{value:"6. Troubleshooting Guide for Whisper Integration",id:"6-troubleshooting-guide-for-whisper-integration",level:3},{value:"Audio Format Issues",id:"audio-format-issues",level:4},{value:"Performance Issues",id:"performance-issues",level:4},{value:"Accuracy Issues",id:"accuracy-issues",level:4},{value:"ROS Integration Issues",id:"ros-integration-issues",level:4},{value:"Assessment Criteria",id:"assessment-criteria",level:2}];function d(n){const e={code:"code",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:"Chapter 1: Voice-to-Action Interfaces"}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Integrate OpenAI Whisper for speech-to-text conversion"}),"\n",(0,t.jsx)(e.li,{children:"Convert voice commands into structured task intents"}),"\n",(0,t.jsx)(e.li,{children:"Publish intents to ROS 2 topics/actions"}),"\n",(0,t.jsx)(e.li,{children:"Create voice command processing pipelines"}),"\n",(0,t.jsx)(e.li,{children:"Implement troubleshooting strategies for Whisper integration"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"key-topics",children:"Key Topics"}),"\n",(0,t.jsx)(e.h3,{id:"1-openai-whisper-integration",children:"1. OpenAI Whisper Integration"}),"\n",(0,t.jsx)(e.p,{children:"OpenAI Whisper is a state-of-the-art speech recognition model that can convert spoken language into text. For robotics applications, Whisper provides a robust foundation for voice command processing."}),"\n",(0,t.jsx)(e.h4,{id:"installation-and-setup",children:"Installation and Setup"}),"\n",(0,t.jsx)(e.p,{children:"To get started with Whisper in your robotics project:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"pip install openai-whisper\n# Or for GPU acceleration\npip install openai-whisper[cuda]\n"})}),"\n",(0,t.jsx)(e.h4,{id:"basic-whisper-implementation",children:"Basic Whisper Implementation"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import whisper\nimport torch\nimport rospy\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\n\nclass WhisperVoiceProcessor:\n    def __init__(self, model_size="base"):\n        # Load the Whisper model\n        self.model = whisper.load_model(model_size)\n\n        # Initialize ROS\n        rospy.init_node(\'whisper_voice_processor\')\n\n        # Subscribe to audio data\n        self.audio_sub = rospy.Subscriber(\'/audio_input\', AudioData, self.audio_callback)\n\n        # Publisher for recognized text\n        self.text_pub = rospy.Publisher(\'/voice_commands\', String, queue_size=10)\n\n        rospy.loginfo("Whisper voice processor initialized")\n\n    def audio_callback(self, audio_data):\n        """Process incoming audio data"""\n        # Convert audio data to numpy array\n        audio_array = self.audio_to_numpy(audio_data)\n\n        # Transcribe the audio\n        result = self.model.transcribe(audio_array)\n        recognized_text = result[\'text\']\n\n        # Publish the recognized text\n        self.text_pub.publish(recognized_text)\n        rospy.loginfo(f"Recognized: {recognized_text}")\n\n    def audio_to_numpy(self, audio_data):\n        """Convert ROS AudioData to numpy array"""\n        # Implementation depends on audio format\n        # This is a simplified example\n        import numpy as np\n        audio_array = np.frombuffer(audio_data.data, dtype=np.int16)\n        return audio_array.astype(np.float32) / 32768.0\n\nif __name__ == \'__main__\':\n    processor = WhisperVoiceProcessor()\n    rospy.spin()\n'})}),"\n",(0,t.jsx)(e.h3,{id:"2-speech-to-text-conversion-processes",children:"2. Speech-to-Text Conversion Processes"}),"\n",(0,t.jsx)(e.p,{children:"The speech-to-text conversion process involves several steps to ensure accurate recognition:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Audio Preprocessing"}),": Clean and normalize audio input"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Feature Extraction"}),": Extract relevant features from the audio signal"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Model Inference"}),": Use the Whisper model to transcribe speech"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Post-processing"}),": Clean and format the transcribed text"]}),"\n"]}),"\n",(0,t.jsx)(e.h4,{id:"audio-preprocessing-example",children:"Audio Preprocessing Example"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom scipy import signal\n\ndef preprocess_audio(audio_data, sample_rate=16000):\n    """Preprocess audio data for better recognition"""\n    # Normalize audio\n    audio_data = audio_data / np.max(np.abs(audio_data))\n\n    # Apply pre-emphasis filter\n    audio_data = signal.lfilter([1, -0.97], [1], audio_data)\n\n    # Trim silence at the beginning and end\n    threshold = 0.01  # 1% of max amplitude\n    non_silent = np.where(np.abs(audio_data) > threshold)[0]\n\n    if len(non_silent) > 0:\n        start = non_silent[0]\n        end = non_silent[-1] + 1\n        audio_data = audio_data[start:end]\n\n    return audio_data\n'})}),"\n",(0,t.jsx)(e.h3,{id:"3-voice-command-processing-pipelines",children:"3. Voice Command Processing Pipelines"}),"\n",(0,t.jsx)(e.p,{children:"Creating an effective voice command processing pipeline involves multiple stages:"}),"\n",(0,t.jsx)(e.h4,{id:"pipeline-architecture",children:"Pipeline Architecture"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class VoiceCommandPipeline:\n    def __init__(self):\n        self.whisper_model = whisper.load_model("base")\n        self.intent_classifier = IntentClassifier()\n        self.ros_publisher = ROSPublisher()\n\n    def process_voice_command(self, audio_data):\n        """Complete pipeline for processing voice commands"""\n        # Step 1: Convert audio to text\n        text = self.audio_to_text(audio_data)\n\n        # Step 2: Classify intent\n        intent = self.intent_classifier.classify(text)\n\n        # Step 3: Execute ROS action\n        self.ros_publisher.publish_intent(intent)\n\n        return intent\n\n    def audio_to_text(self, audio_data):\n        """Convert audio to text using Whisper"""\n        # Preprocess audio\n        processed_audio = preprocess_audio(audio_data)\n\n        # Transcribe using Whisper\n        result = self.whisper_model.transcribe(processed_audio)\n        return result[\'text\']\n'})}),"\n",(0,t.jsx)(e.h3,{id:"4-structured-task-intents-mapping",children:"4. Structured Task Intents Mapping"}),"\n",(0,t.jsx)(e.p,{children:"To convert voice commands into actionable tasks, we need to map recognized text to structured intents:"}),"\n",(0,t.jsx)(e.h4,{id:"intent-mapping-example",children:"Intent Mapping Example"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import re\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\n\n@dataclass\nclass TaskIntent:\n    action: str\n    parameters: Dict[str, any]\n    confidence: float\n\nclass IntentClassifier:\n    def __init__(self):\n        self.intent_patterns = {\n            'move_to_location': [\n                r'move to (.+)',\n                r'go to (.+)',\n                r'go to the (.+)',\n                r'walk to (.+)'\n            ],\n            'pick_object': [\n                r'pick up (.+)',\n                r'grab (.+)',\n                r'get the (.+)',\n                r'take (.+)'\n            ],\n            'place_object': [\n                r'place (.+) at (.+)',\n                r'put (.+) on (.+)',\n                r'drop (.+) at (.+)'\n            ],\n            'stop_robot': [\n                r'stop',\n                r'freeze',\n                r'hold position'\n            ]\n        }\n\n    def classify(self, text: str) -> TaskIntent:\n        \"\"\"Classify text into structured intent\"\"\"\n        text_lower = text.lower().strip()\n\n        for action, patterns in self.intent_patterns.items():\n            for pattern in patterns:\n                match = re.search(pattern, text_lower)\n                if match:\n                    # Extract parameters based on pattern groups\n                    params = {\n                        f'param_{i+1}': param\n                        for i, param in enumerate(match.groups())\n                    }\n                    return TaskIntent(action=action, parameters=params, confidence=0.9)\n\n        # If no pattern matches, return a generic command\n        return TaskIntent(action='unknown', parameters={'text': text}, confidence=0.0)\n"})}),"\n",(0,t.jsx)(e.h3,{id:"5-practical-implementation-voice-to-action-system",children:"5. Practical Implementation: Voice-to-Action System"}),"\n",(0,t.jsx)(e.p,{children:"Here's a complete implementation of a voice-to-action system:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rospy\nimport whisper\nimport numpy as np\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom move_base_msgs.msg import MoveBaseActionGoal\nimport json\n\nclass VoiceToActionSystem:\n    def __init__(self):\n        rospy.init_node(\'voice_to_action_system\')\n\n        # Load Whisper model\n        self.model = whisper.load_model("base")\n\n        # Subscribe to voice commands\n        self.voice_sub = rospy.Subscriber(\'/voice_input\', String, self.voice_callback)\n\n        # Publishers for different action types\n        self.nav_pub = rospy.Publisher(\'/move_base/goal\', MoveBaseActionGoal, queue_size=10)\n        self.command_pub = rospy.Publisher(\'/robot_commands\', String, queue_size=10)\n\n        # Location mapping for navigation commands\n        self.location_map = {\n            \'kitchen\': [1.0, 2.0, 0.0],  # x, y, theta\n            \'living room\': [3.0, 1.0, 1.57],\n            \'bedroom\': [0.0, 4.0, 3.14],\n            \'office\': [2.5, 3.5, -1.57]\n        }\n\n        rospy.loginfo("Voice-to-Action system initialized")\n\n    def voice_callback(self, msg):\n        """Process incoming voice command"""\n        command = msg.data.lower()\n\n        # Process the command and execute appropriate action\n        if self.is_navigation_command(command):\n            self.handle_navigation_command(command)\n        elif self.is_action_command(command):\n            self.handle_action_command(command)\n        else:\n            rospy.logwarn(f"Unknown command: {command}")\n\n    def is_navigation_command(self, command):\n        """Check if command is a navigation command"""\n        navigation_keywords = [\'go to\', \'move to\', \'walk to\', \'navigate to\']\n        return any(keyword in command for keyword in navigation_keywords)\n\n    def is_action_command(self, command):\n        """Check if command is an action command"""\n        action_keywords = [\'pick\', \'grab\', \'take\', \'place\', \'put\', \'drop\']\n        return any(keyword in command for keyword in action_keywords)\n\n    def handle_navigation_command(self, command):\n        """Handle navigation commands"""\n        # Extract location from command\n        for location_name, location_coords in self.location_map.items():\n            if location_name in command:\n                self.navigate_to_location(location_name, location_coords)\n                return\n\n        rospy.logwarn(f"Unknown location in command: {command}")\n\n    def navigate_to_location(self, location_name, coords):\n        """Navigate to a specific location"""\n        goal = MoveBaseActionGoal()\n        goal.header.stamp = rospy.Time.now()\n        goal.header.frame_id = "map"\n\n        goal.goal.target_pose.header.frame_id = "map"\n        goal.goal.target_pose.pose.position.x = coords[0]\n        goal.goal.target_pose.pose.position.y = coords[1]\n        goal.goal.target_pose.pose.orientation.z = np.sin(coords[2] / 2)\n        goal.goal.target_pose.pose.orientation.w = np.cos(coords[2] / 2)\n\n        self.nav_pub.publish(goal)\n        rospy.loginfo(f"Navigating to {location_name}")\n\n    def handle_action_command(self, command):\n        """Handle action commands"""\n        action_msg = String()\n        action_msg.data = command\n        self.command_pub.publish(action_msg)\n        rospy.loginfo(f"Action command published: {command}")\n\n    def run(self):\n        """Run the voice-to-action system"""\n        rospy.spin()\n\nif __name__ == \'__main__\':\n    system = VoiceToActionSystem()\n    system.run()\n'})}),"\n",(0,t.jsx)(e.h3,{id:"6-troubleshooting-guide-for-whisper-integration",children:"6. Troubleshooting Guide for Whisper Integration"}),"\n",(0,t.jsx)(e.p,{children:"Common issues and solutions when integrating Whisper:"}),"\n",(0,t.jsx)(e.h4,{id:"audio-format-issues",children:"Audio Format Issues"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Problem"}),": Whisper expects audio in specific formats"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Solution"}),": Ensure audio is at the correct sample rate (16kHz recommended)"]}),"\n"]}),"\n",(0,t.jsx)(e.h4,{id:"performance-issues",children:"Performance Issues"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Problem"}),": Slow transcription on CPU"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Solution"}),": Use GPU acceleration or smaller Whisper models"]}),"\n"]}),"\n",(0,t.jsx)(e.h4,{id:"accuracy-issues",children:"Accuracy Issues"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Problem"}),": Poor recognition in noisy environments"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Solution"}),": Implement audio preprocessing and noise reduction"]}),"\n"]}),"\n",(0,t.jsx)(e.h4,{id:"ros-integration-issues",children:"ROS Integration Issues"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Problem"}),": Latency in voice processing pipeline"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Solution"}),": Optimize audio buffering and implement asynchronous processing"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"assessment-criteria",children:"Assessment Criteria"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Students can successfully integrate OpenAI Whisper for speech-to-text conversion"}),"\n",(0,t.jsx)(e.li,{children:"Students can convert voice commands into structured task intents"}),"\n",(0,t.jsx)(e.li,{children:"Students can publish intents to ROS 2 topics/actions"}),"\n",(0,t.jsx)(e.li,{children:"Students can create effective voice command processing pipelines"}),"\n",(0,t.jsx)(e.li,{children:"Students can troubleshoot common issues in Whisper integration"}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453(n,e,i){i.d(e,{R:()=>a,x:()=>r});var o=i(6540);const t={},s=o.createContext(t);function a(n){const e=o.useContext(s);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:a(n.components),o.createElement(s.Provider,{value:e},n.children)}}}]);