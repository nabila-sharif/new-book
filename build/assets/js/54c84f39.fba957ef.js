"use strict";(self.webpackChunkfrontend_book=self.webpackChunkfrontend_book||[]).push([[7507],{6471(e,n,s){s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"physical-ai-humanoid-robotics/chapter2-ros2-python-communication","title":"Chapter 2: Python-based AI Agent - From Decision to Action","description":"This chapter covers using ROS 2 communication with Python using rclpy to create AI agents that can translate decisions into robot actions. We\'ll explore how to implement publishers, subscribers, and services to create effective AI agents that can interact with the physical world.","source":"@site/docs/physical-ai-humanoid-robotics/chapter2-ros2-python-communication.md","sourceDirName":"physical-ai-humanoid-robotics","slug":"/physical-ai-humanoid-robotics/chapter2-ros2-python-communication","permalink":"/docs/physical-ai-humanoid-robotics/chapter2-ros2-python-communication","draft":false,"unlisted":false,"editUrl":"https://github.com/nabila-sharif/AI---Humanoid-Robotics-Book/tree/main/frontend_book/docs/physical-ai-humanoid-robotics/chapter2-ros2-python-communication.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: ROS 2 Fundamentals - The Robotic Nervous System","permalink":"/docs/physical-ai-humanoid-robotics/chapter1-ros2-fundamentals"},"next":{"title":"Chapter 3: Humanoid Robot Modeler - Defining Structure with URDF","permalink":"/docs/physical-ai-humanoid-robotics/chapter3-urdf-humanoid-structure"}}');var i=s(4848),a=s(8453);const o={sidebar_position:3},r="Chapter 2: Python-based AI Agent - From Decision to Action",l={},c=[{value:"Python and ROS 2 Integration",id:"python-and-ros-2-integration",level:2},{value:"Installing rclpy",id:"installing-rclpy",level:3},{value:"Creating AI Agents with rclpy",id:"creating-ai-agents-with-rclpy",level:2},{value:"Basic AI Agent Structure",id:"basic-ai-agent-structure",level:3},{value:"Communication Patterns with rclpy",id:"communication-patterns-with-rclpy",level:2},{value:"Publishers",id:"publishers",level:3},{value:"Subscribers",id:"subscribers",level:3},{value:"Services",id:"services",level:3},{value:"Advanced AI Agent Patterns",id:"advanced-ai-agent-patterns",level:2},{value:"State Management",id:"state-management",level:3},{value:"Integration with Machine Learning Models",id:"integration-with-machine-learning-models",level:3},{value:"Complete Example: AI Decision to Robot Action Flow",id:"complete-example-ai-decision-to-robot-action-flow",level:2},{value:"Hands-on Exercise: Creating Your Own Python-based AI Agent",id:"hands-on-exercise-creating-your-own-python-based-ai-agent",level:2},{value:"Exercise 1: Basic AI Agent",id:"exercise-1-basic-ai-agent",level:3},{value:"Exercise 2: Enhanced Decision Making",id:"exercise-2-enhanced-decision-making",level:3},{value:"Exercise 3: Integration with Machine Learning",id:"exercise-3-integration-with-machine-learning",level:3},{value:"Solution Template",id:"solution-template",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-2-python-based-ai-agent---from-decision-to-action",children:"Chapter 2: Python-based AI Agent - From Decision to Action"})}),"\n",(0,i.jsx)(n.p,{children:"This chapter covers using ROS 2 communication with Python using rclpy to create AI agents that can translate decisions into robot actions. We'll explore how to implement publishers, subscribers, and services to create effective AI agents that can interact with the physical world."}),"\n",(0,i.jsx)(n.h2,{id:"python-and-ros-2-integration",children:"Python and ROS 2 Integration"}),"\n",(0,i.jsx)(n.p,{children:"Python is one of the primary languages used in ROS 2 development. The rclpy package provides a Python client library for ROS 2, making it accessible for AI researchers and developers who prefer Python for its rich ecosystem of machine learning and AI libraries."}),"\n",(0,i.jsx)(n.h3,{id:"installing-rclpy",children:"Installing rclpy"}),"\n",(0,i.jsx)(n.p,{children:"rclpy is typically installed as part of a ROS 2 distribution. If you're setting up your environment, make sure to source your ROS 2 installation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"source /opt/ros/humble/setup.bash  # or your ROS 2 distribution\n"})}),"\n",(0,i.jsx)(n.h2,{id:"creating-ai-agents-with-rclpy",children:"Creating AI Agents with rclpy"}),"\n",(0,i.jsx)(n.p,{children:"An AI agent in the ROS 2 context is a node that receives sensor data, processes it using AI algorithms, and publishes control commands to actuators. This creates a complete decision-making loop that connects AI reasoning with physical action."}),"\n",(0,i.jsx)(n.h3,{id:"basic-ai-agent-structure",children:"Basic AI Agent Structure"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan\nfrom geometry_msgs.msg import Twist\nimport numpy as np\n\nclass AIAgent(Node):\n    def __init__(self):\n        super().__init__('ai_agent')\n\n        # Subscribe to sensor data\n        self.subscription = self.create_subscription(\n            LaserScan,\n            'scan',\n            self.sensor_callback,\n            10)\n\n        # Publish control commands\n        self.publisher = self.create_publisher(\n            Twist,\n            'cmd_vel',\n            10)\n\n        # Timer for AI decision-making\n        self.timer = self.create_timer(0.1, self.ai_decision_loop)\n\n        self.latest_sensor_data = None\n        self.get_logger().info('AI Agent initialized')\n\n    def sensor_callback(self, msg):\n        # Store latest sensor data for AI processing\n        self.latest_sensor_data = msg.ranges\n\n    def ai_decision_loop(self):\n        if self.latest_sensor_data is not None:\n            # Apply AI algorithm to sensor data\n            control_command = self.make_decision(self.latest_sensor_data)\n\n            # Publish the control command\n            self.publisher.publish(control_command)\n\n    def make_decision(self, sensor_data):\n        # Simple AI algorithm: avoid obstacles\n        msg = Twist()\n\n        if min(sensor_data) > 1.0:  # No obstacles nearby\n            msg.linear.x = 0.5  # Move forward\n            msg.angular.z = 0.0\n        else:  # Obstacle detected\n            msg.linear.x = 0.0\n            msg.angular.z = 0.5  # Turn right\n\n        return msg\n\ndef main(args=None):\n    rclpy.init(args=args)\n    ai_agent = AIAgent()\n    rclpy.spin(ai_agent)\n    ai_agent.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"communication-patterns-with-rclpy",children:"Communication Patterns with rclpy"}),"\n",(0,i.jsx)(n.h3,{id:"publishers",children:"Publishers"}),"\n",(0,i.jsx)(n.p,{children:"Publishers send data to topics. In an AI agent context, publishers are used to send control commands to robot actuators."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Creating a publisher\nself.cmd_publisher = self.create_publisher(Twist, 'cmd_vel', 10)\n\n# Publishing a message\nmsg = Twist()\nmsg.linear.x = 0.5\nmsg.angular.z = 0.1\nself.cmd_publisher.publish(msg)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"subscribers",children:"Subscribers"}),"\n",(0,i.jsx)(n.p,{children:"Subscribers receive data from topics. In an AI agent context, subscribers are used to receive sensor data from robot sensors."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Creating a subscriber\nself.sensor_subscriber = self.create_subscription(\n    LaserScan,\n    'scan',\n    self.process_sensor_data,\n    10)\n\n# Processing received data\ndef process_sensor_data(self, msg):\n    # Process sensor data with AI algorithm\n    self.handle_sensor_input(msg)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"services",children:"Services"}),"\n",(0,i.jsx)(n.p,{children:"Services provide request/reply communication. In an AI agent context, services can be used for high-level commands or to request information."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Creating a service server\nself.service = self.create_service(\n    Trigger,\n    \'ai_decision_service\',\n    self.handle_decision_request)\n\n# Handling service requests\ndef handle_decision_request(self, request, response):\n    if self.ai_model.is_ready():\n        response.success = True\n        response.message = "AI decision ready"\n    else:\n        response.success = False\n        response.message = "AI model not ready"\n    return response\n'})}),"\n",(0,i.jsx)(n.h2,{id:"advanced-ai-agent-patterns",children:"Advanced AI Agent Patterns"}),"\n",(0,i.jsx)(n.h3,{id:"state-management",children:"State Management"}),"\n",(0,i.jsx)(n.p,{children:"AI agents often need to maintain state between decisions. Here's how to implement state management in your agent:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class StatefulAIAgent(Node):\n    def __init__(self):\n        super().__init__('stateful_ai_agent')\n        self.agent_state = 'SEARCHING'  # Possible states: SEARCHING, APPROACHING, AVOIDING\n        self.target_location = None\n        self.memory = []  # Store past decisions\n\n        # Setup subscribers, publishers, etc.\n\n    def update_state(self, sensor_data):\n        # Update agent state based on sensor data and current state\n        if self.agent_state == 'SEARCHING' and self.detect_target(sensor_data):\n            self.agent_state = 'APPROACHING'\n            self.get_logger().info('State changed to APPROACHING')\n"})}),"\n",(0,i.jsx)(n.h3,{id:"integration-with-machine-learning-models",children:"Integration with Machine Learning Models"}),"\n",(0,i.jsx)(n.p,{children:"AI agents often use pre-trained machine learning models for decision-making:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import tensorflow as tf  # or your preferred ML framework\n\nclass MLIARobot(Node):\n    def __init__(self):\n        super().__init__('ml_ai_robot')\n\n        # Load pre-trained model\n        self.ml_model = tf.keras.models.load_model('path/to/model')\n\n        # Setup ROS 2 communication\n        self.subscription = self.create_subscription(\n            Image,\n            'camera/image_raw',\n            self.image_callback,\n            10)\n\n    def image_callback(self, msg):\n        # Convert ROS image to format suitable for ML model\n        image_data = self.ros_image_to_ml_format(msg)\n\n        # Get prediction from ML model\n        prediction = self.ml_model.predict(image_data)\n\n        # Convert prediction to robot action\n        action = self.prediction_to_action(prediction)\n\n        # Publish action\n        self.publish_action(action)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"complete-example-ai-decision-to-robot-action-flow",children:"Complete Example: AI Decision to Robot Action Flow"}),"\n",(0,i.jsx)(n.p,{children:"Let's look at a complete example that demonstrates the full flow from AI decision-making to robot action. This example implements a simple object recognition and navigation system:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass ObjectNavigationAgent(Node):\n    def __init__(self):\n        super().__init__(\'object_navigation_agent\')\n\n        # Initialize CV bridge to convert ROS images to OpenCV format\n        self.bridge = CvBridge()\n\n        # Subscribe to camera and laser scanner\n        self.image_subscription = self.create_subscription(\n            Image,\n            \'camera/image_raw\',\n            self.image_callback,\n            10)\n\n        self.scan_subscription = self.create_subscription(\n            LaserScan,\n            \'scan\',\n            self.scan_callback,\n            10)\n\n        # Publisher for robot commands\n        self.cmd_publisher = self.create_publisher(Twist, \'cmd_vel\', 10)\n\n        # Publisher for object detection status\n        self.status_publisher = self.create_publisher(String, \'object_status\', 10)\n\n        # Timer for decision-making loop\n        self.timer = self.create_timer(0.1, self.decision_loop)\n\n        # State variables\n        self.latest_image = None\n        self.latest_scan = None\n        self.detected_object = None\n        self.navigation_state = \'SEARCHING\'  # SEARCHING, NAVIGATING, AVOIDING\n\n        self.get_logger().info(\'Object Navigation Agent initialized\')\n\n    def image_callback(self, msg):\n        """Process camera images to detect objects"""\n        try:\n            # Convert ROS image to OpenCV format\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n\n            # Simple color-based object detection (red object)\n            hsv = cv2.cvtColor(cv_image, cv2.COLOR_BGR2HSV)\n\n            # Define range for red color\n            lower_red = np.array([0, 50, 50])\n            upper_red = np.array([10, 255, 255])\n            mask1 = cv2.inRange(hsv, lower_red, upper_red)\n\n            lower_red = np.array([170, 50, 50])\n            upper_red = np.array([180, 255, 255])\n            mask2 = cv2.inRange(hsv, lower_red, upper_red)\n\n            mask = mask1 + mask2\n\n            # Find contours\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            if contours:\n                # Find the largest contour\n                largest_contour = max(contours, key=cv2.contourArea)\n                if cv2.contourArea(largest_contour) > 500:  # Minimum area threshold\n                    # Calculate center of the object\n                    M = cv2.moments(largest_contour)\n                    if M["m00"] != 0:\n                        cx = int(M["m10"] / M["m00"])\n                        cy = int(M["m01"] / M["m00"])\n\n                        # Normalize position (0 = left, 1 = right)\n                        img_width = cv_image.shape[1]\n                        object_position = cx / img_width\n\n                        self.detected_object = {\n                            \'position\': object_position,\n                            \'area\': cv2.contourArea(largest_contour)\n                        }\n\n                        # Update navigation state\n                        self.navigation_state = \'NAVIGATING\'\n\n                        # Publish object status\n                        status_msg = String()\n                        status_msg.data = f"Red object detected at position {object_position:.2f}"\n                        self.status_publisher.publish(status_msg)\n            else:\n                self.detected_object = None\n                self.navigation_state = \'SEARCHING\'\n\n            self.latest_image = cv_image\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def scan_callback(self, msg):\n        """Process laser scan data for obstacle detection"""\n        self.latest_scan = msg.ranges\n\n    def decision_loop(self):\n        """Main AI decision-making loop"""\n        if self.navigation_state == \'SEARCHING\':\n            self.search_behavior()\n        elif self.navigation_state == \'NAVIGATING\':\n            if self.detected_object:\n                self.navigate_to_object()\n            else:\n                self.navigation_state = \'SEARCHING\'\n        elif self.navigation_state == \'AVOIDING\':\n            self.avoid_obstacles()\n\n    def search_behavior(self):\n        """Behavior when searching for objects"""\n        cmd = Twist()\n        cmd.linear.x = 0.2  # Move forward slowly\n        cmd.angular.z = 0.3  # Rotate slowly to scan environment\n        self.cmd_publisher.publish(cmd)\n\n    def navigate_to_object(self):\n        """Navigate toward the detected object"""\n        if not self.detected_object or not self.latest_scan:\n            return\n\n        cmd = Twist()\n\n        # Check for obstacles\n        if self.latest_scan:\n            min_distance = min([d for d in self.latest_scan if not np.isnan(d) and d > 0.1])\n            if min_distance < 0.5:  # Obstacle too close\n                self.navigation_state = \'AVOIDING\'\n                cmd.linear.x = 0.0\n                cmd.angular.z = 0.5  # Turn away from obstacle\n                self.cmd_publisher.publish(cmd)\n                return\n\n        # Navigate toward object\n        object_pos = self.detected_object[\'position\']\n\n        if object_pos < 0.4:  # Object is to the left\n            cmd.angular.z = 0.3\n        elif object_pos > 0.6:  # Object is to the right\n            cmd.angular.z = -0.3\n        else:  # Object is centered\n            cmd.linear.x = 0.3  # Move forward toward object\n\n        self.cmd_publisher.publish(cmd)\n\n    def avoid_obstacles(self):\n        """Simple obstacle avoidance behavior"""\n        if not self.latest_scan:\n            return\n\n        cmd = Twist()\n\n        # Simple wall-following behavior\n        left_scan = self.latest_scan[0:90]  # Left quarter\n        front_scan = self.latest_scan[90:270]  # Front half\n        right_scan = self.latest_scan[270:360]  # Right quarter\n\n        min_front = min([d for d in front_scan if not np.isnan(d) and d > 0.1])\n        min_left = min([d for d in left_scan if not np.isnan(d) and d > 0.1])\n        min_right = min([d for d in right_scan if not np.isnan(d) and d > 0.1])\n\n        if min_front > 0.8:  # Path is clear\n            cmd.linear.x = 0.3\n            cmd.angular.z = 0.0\n            self.navigation_state = \'NAVIGATING\'  # Return to navigation\n        elif min_left > min_right:  # More space on left\n            cmd.angular.z = 0.5\n        else:  # More space on right\n            cmd.angular.z = -0.5\n\n        self.cmd_publisher.publish(cmd)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    agent = ObjectNavigationAgent()\n    rclpy.spin(agent)\n    agent.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"hands-on-exercise-creating-your-own-python-based-ai-agent",children:"Hands-on Exercise: Creating Your Own Python-based AI Agent"}),"\n",(0,i.jsx)(n.p,{children:"Now it's time to create your own AI agent. Follow these steps to build a complete system:"}),"\n",(0,i.jsx)(n.h3,{id:"exercise-1-basic-ai-agent",children:"Exercise 1: Basic AI Agent"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Create a new Python file called ",(0,i.jsx)(n.code,{children:"my_ai_agent.py"})]}),"\n",(0,i.jsx)(n.li,{children:"Implement a basic AI agent that subscribes to sensor data"}),"\n",(0,i.jsx)(n.li,{children:"Implement a simple decision-making algorithm"}),"\n",(0,i.jsx)(n.li,{children:"Publish commands to control the robot"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"exercise-2-enhanced-decision-making",children:"Exercise 2: Enhanced Decision Making"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Modify your agent to handle multiple sensor inputs"}),"\n",(0,i.jsx)(n.li,{children:"Implement state management to remember past decisions"}),"\n",(0,i.jsx)(n.li,{children:"Add a service that allows external systems to query the agent's state"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"exercise-3-integration-with-machine-learning",children:"Exercise 3: Integration with Machine Learning"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Integrate a simple machine learning model (you can use scikit-learn or TensorFlow)"}),"\n",(0,i.jsx)(n.li,{children:"Train the model to recognize patterns in sensor data"}),"\n",(0,i.jsx)(n.li,{children:"Use the model's predictions to influence the robot's behavior"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"solution-template",children:"Solution Template"}),"\n",(0,i.jsx)(n.p,{children:"Here's a template to get you started:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan  # Add other message types as needed\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import String\n\nclass MyAIAgent(Node):\n    def __init__(self):\n        super().__init__('my_ai_agent')\n\n        # TODO: Setup your subscribers, publishers, and timers\n        # TODO: Initialize your AI model or decision variables\n\n    def sensor_callback(self, msg):\n        # TODO: Process sensor data\n        pass\n\n    def ai_decision_callback(self):\n        # TODO: Implement your AI decision-making logic\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    agent = MyAIAgent()\n    rclpy.spin(agent)\n    agent.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"In this chapter, we've explored how to create Python-based AI agents using rclpy and ROS 2. You've learned about:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"How to implement publishers, subscribers, and services using rclpy"}),"\n",(0,i.jsx)(n.li,{children:"The complete flow from AI decision-making to robot action"}),"\n",(0,i.jsx)(n.li,{children:"Practical examples of AI agents that process sensor data and control robot behavior"}),"\n",(0,i.jsx)(n.li,{children:"Hands-on exercises to build your own AI agents"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"These skills enable you to create sophisticated AI systems that can interact with the physical world through robots. The next chapter will cover how to model robot structure using URDF, which is essential for simulation and control."}),"\n",(0,i.jsx)(n.p,{children:"This chapter demonstrates the complete flow from AI decision-making to robot action, showing how Python and rclpy enable the creation of sophisticated AI agents that can process information and control robot behavior effectively."})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,s){s.d(n,{R:()=>o,x:()=>r});var t=s(6540);const i={},a=t.createContext(i);function o(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);