"use strict";(self.webpackChunkfrontend_book=self.webpackChunkfrontend_book||[]).push([[1484],{8453(e,n,i){i.d(n,{R:()=>o,x:()=>s});var t=i(6540);const a={},r=t.createContext(a);function o(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),t.createElement(r.Provider,{value:n},e.children)}},9831(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-2/chapter-3-content","title":"Chapter 3: Sensor Simulation & Fidelity","description":"Introduction to Sensor Simulation in Digital Twins","source":"@site/docs/module-2/chapter-3-content.md","sourceDirName":"module-2","slug":"/module-2/chapter-3-content","permalink":"/docs/module-2/chapter-3-content","draft":false,"unlisted":false,"editUrl":"https://github.com/nabila-sharif/AI---Humanoid-Robotics-Book/tree/main/frontend_book/docs/module-2/chapter-3-content.md","tags":[],"version":"current","frontMatter":{},"sidebar":"module2Sidebar","previous":{"title":"Chapter 2: High-Fidelity Environments with Unity","permalink":"/docs/module-2/chapter-2-content"}}');var a=i(4848),r=i(8453);const o={},s="Chapter 3: Sensor Simulation & Fidelity",l={},c=[{value:"Introduction to Sensor Simulation in Digital Twins",id:"introduction-to-sensor-simulation-in-digital-twins",level:2},{value:"LiDAR Simulation and Noise Characteristics",id:"lidar-simulation-and-noise-characteristics",level:2},{value:"LiDAR Physics in Simulation",id:"lidar-physics-in-simulation",level:3},{value:"LiDAR Simulation Implementation",id:"lidar-simulation-implementation",level:3},{value:"Noise Modeling for LiDAR Sensors",id:"noise-modeling-for-lidar-sensors",level:3},{value:"Statistical Noise Models",id:"statistical-noise-models",level:4},{value:"Systematic Errors",id:"systematic-errors",level:4},{value:"Advanced LiDAR Simulation Features",id:"advanced-lidar-simulation-features",level:3},{value:"Depth Camera and RGB-D Simulation",id:"depth-camera-and-rgb-d-simulation",level:2},{value:"Depth Camera Fundamentals",id:"depth-camera-fundamentals",level:3},{value:"Depth Camera Implementation",id:"depth-camera-implementation",level:3},{value:"RGB-D Sensor Pipeline",id:"rgb-d-sensor-pipeline",level:3},{value:"IMU Modeling and Drift",id:"imu-modeling-and-drift",level:2},{value:"IMU Fundamentals in Simulation",id:"imu-fundamentals-in-simulation",level:3},{value:"IMU Simulation Implementation",id:"imu-simulation-implementation",level:3},{value:"Sensor Synchronization and Data Accuracy",id:"sensor-synchronization-and-data-accuracy",level:2},{value:"Time Synchronization Challenges",id:"time-synchronization-challenges",level:3},{value:"Synchronization Implementation",id:"synchronization-implementation",level:3},{value:"Evaluating Sensor Realism and Limitations",id:"evaluating-sensor-realism-and-limitations",level:2},{value:"Sensor Fidelity Metrics",id:"sensor-fidelity-metrics",level:3},{value:"Accuracy Metrics",id:"accuracy-metrics",level:4},{value:"Performance Metrics",id:"performance-metrics",level:4},{value:"Validation Against Real Hardware",id:"validation-against-real-hardware",level:3},{value:"Best Practices for Sensor Simulation",id:"best-practices-for-sensor-simulation",level:2},{value:"Modeling Guidelines",id:"modeling-guidelines",level:3},{value:"Performance Considerations",id:"performance-considerations",level:3},{value:"Integration Strategies",id:"integration-strategies",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-3-sensor-simulation--fidelity",children:"Chapter 3: Sensor Simulation & Fidelity"})}),"\n",(0,a.jsx)(n.h2,{id:"introduction-to-sensor-simulation-in-digital-twins",children:"Introduction to Sensor Simulation in Digital Twins"}),"\n",(0,a.jsx)(n.p,{children:"Sensor simulation is a critical component of digital twin systems for robotics, as it provides the virtual robot with environmental perception capabilities that mirror those of its physical counterpart. Accurate sensor simulation enables:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Development and testing of perception algorithms in a safe environment"}),"\n",(0,a.jsx)(n.li,{children:"Training of machine learning models with synthetic data"}),"\n",(0,a.jsx)(n.li,{children:"Validation of navigation and control systems"}),"\n",(0,a.jsx)(n.li,{children:"Evaluation of robot performance under various environmental conditions"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"The fidelity of sensor simulation directly impacts the transferability of algorithms from simulation to reality, making it essential to model sensor characteristics, noise, and limitations accurately."}),"\n",(0,a.jsx)(n.h2,{id:"lidar-simulation-and-noise-characteristics",children:"LiDAR Simulation and Noise Characteristics"}),"\n",(0,a.jsx)(n.h3,{id:"lidar-physics-in-simulation",children:"LiDAR Physics in Simulation"}),"\n",(0,a.jsx)(n.p,{children:"LiDAR (Light Detection and Ranging) sensors in simulation are typically implemented using raycasting techniques that mimic the behavior of real laser beams. The simulation must account for:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Ray casting"}),": Virtual laser beams projected from the sensor origin"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Distance measurement"}),": Calculation of distances to nearest obstacles"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Angular resolution"}),": Horizontal and vertical beam spacing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Range limitations"}),": Maximum and minimum detection distances"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"lidar-simulation-implementation",children:"LiDAR Simulation Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'using UnityEngine;\nusing System.Collections.Generic;\n\npublic class LIDARSimulator : MonoBehaviour\n{\n    [Header("LIDAR Configuration")]\n    public int horizontalRays = 360;\n    public int verticalRays = 16;\n    public float minAngle = -30f;\n    public float maxAngle = 15f;\n    public float maxRange = 20.0f;\n    public float minRange = 0.1f;\n    public LayerMask detectionMask = -1;\n\n    [Header("Noise Parameters")]\n    public float distanceNoiseStdDev = 0.02f;  // 2cm standard deviation\n    public float angularNoiseStdDev = 0.001f;  // Small angular error\n\n    private List<float> scanData;\n    private float[] verticalAngles;\n\n    void Start()\n    {\n        InitializeLIDAR();\n    }\n\n    void InitializeLIDAR()\n    {\n        scanData = new List<float>();\n        CalculateVerticalAngles();\n    }\n\n    void CalculateVerticalAngles()\n    {\n        verticalAngles = new float[verticalRays];\n        float angleStep = (maxAngle - minAngle) / (verticalRays - 1);\n\n        for (int i = 0; i < verticalRays; i++)\n        {\n            verticalAngles[i] = minAngle + (i * angleStep);\n        }\n    }\n\n    public float[] GetLIDARScan()\n    {\n        scanData.Clear();\n\n        for (int h = 0; h < horizontalRays; h++)\n        {\n            float horizontalAngle = (360.0f / horizontalRays) * h * Mathf.Deg2Rad;\n\n            for (int v = 0; v < verticalRays; v++)\n            {\n                float verticalAngle = verticalAngles[v] * Mathf.Deg2Rad;\n\n                // Calculate ray direction\n                Vector3 rayDirection = CalculateRayDirection(horizontalAngle, verticalAngle);\n\n                // Perform raycast\n                RaycastHit hit;\n                if (Physics.Raycast(transform.position, rayDirection, out hit, maxRange, detectionMask))\n                {\n                    float distance = hit.distance;\n\n                    // Add noise to the measurement\n                    distance = AddDistanceNoise(distance);\n\n                    scanData.Add(distance);\n                }\n                else\n                {\n                    // No obstacle detected within range\n                    scanData.Add(maxRange + 1.0f); // Indicate no return\n                }\n            }\n        }\n\n        return scanData.ToArray();\n    }\n\n    Vector3 CalculateRayDirection(float horizontalAngle, float verticalAngle)\n    {\n        // Calculate the ray direction based on horizontal and vertical angles\n        Vector3 direction = new Vector3(\n            Mathf.Cos(verticalAngle) * Mathf.Cos(horizontalAngle),\n            Mathf.Cos(verticalAngle) * Mathf.Sin(horizontalAngle),\n            Mathf.Sin(verticalAngle)\n        );\n\n        // Transform to world space based on sensor orientation\n        return transform.TransformDirection(direction);\n    }\n\n    float AddDistanceNoise(float distance)\n    {\n        // Add Gaussian noise to distance measurement\n        float noise = RandomGaussian() * distanceNoiseStdDev;\n        float noisyDistance = distance + noise;\n\n        // Ensure distance is within valid range\n        return Mathf.Clamp(noisyDistance, minRange, maxRange + 1.0f);\n    }\n\n    float RandomGaussian()\n    {\n        // Box-Muller transform for Gaussian random number generation\n        float u1 = Random.value;\n        float u2 = Random.value;\n\n        if (u1 < Mathf.Epsilon) u1 = Mathf.Epsilon;\n\n        float gaussian = Mathf.Sqrt(-2.0f * Mathf.Log(u1)) * Mathf.Cos(2.0f * Mathf.PI * u2);\n        return gaussian;\n    }\n}\n'})}),"\n",(0,a.jsx)(n.h3,{id:"noise-modeling-for-lidar-sensors",children:"Noise Modeling for LiDAR Sensors"}),"\n",(0,a.jsx)(n.p,{children:"Real LiDAR sensors exhibit various types of noise and systematic errors:"}),"\n",(0,a.jsx)(n.h4,{id:"statistical-noise-models",children:"Statistical Noise Models"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Gaussian noise"}),": Random measurement errors following a normal distribution"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Intensity-based noise"}),": Distance measurement errors that vary with return intensity"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multi-path interference"}),": Errors due to multiple reflections"]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"systematic-errors",children:"Systematic Errors"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Zero-point calibration errors"}),": Constant offset in measurements"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Scale factor errors"}),": Proportional errors across the measurement range"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Angular misalignment"}),": Errors in beam pointing direction"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"advanced-lidar-simulation-features",children:"Advanced LiDAR Simulation Features"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'public class AdvancedLIDARSimulator : LIDARSimulator\n{\n    [Header("Advanced Noise Models")]\n    public AnimationCurve distanceNoiseCurve;  // Distance-dependent noise\n    public float temperatureCoefficient = 0.001f;  // Temperature effect\n    public float vibrationNoise = 0.01f;  // Vibration-induced errors\n\n    [Header("Environmental Effects")]\n    public float atmosphericAttenuation = 0.001f;  // Fog/rain effect\n    public float dustAttenuation = 0.002f;  // Dust/particle effect\n\n    float AddAdvancedDistanceNoise(float distance, float temperature, float vibration)\n    {\n        // Base noise from distance curve\n        float distanceBasedNoise = distanceNoiseCurve.Evaluate(distance) * distanceNoiseStdDev;\n\n        // Temperature effect\n        float temperatureEffect = temperature * temperatureCoefficient;\n\n        // Vibration effect\n        float vibrationEffect = vibration * vibrationNoise;\n\n        // Combine all noise sources\n        float totalNoise = Mathf.Sqrt(\n            Mathf.Pow(distanceBasedNoise, 2) +\n            Mathf.Pow(temperatureEffect, 2) +\n            Mathf.Pow(vibrationEffect, 2)\n        );\n\n        return distance + RandomGaussian() * totalNoise;\n    }\n\n    float ApplyEnvironmentalEffects(float distance, float atmosphericDensity, float particleDensity)\n    {\n        // Apply atmospheric attenuation\n        float atmosphericFactor = Mathf.Exp(-atmosphericAttenuation * distance * atmosphericDensity);\n\n        // Apply dust/particle attenuation\n        float dustFactor = Mathf.Exp(-dustAttenuation * distance * particleDensity);\n\n        return distance / (atmosphericFactor * dustFactor);\n    }\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"depth-camera-and-rgb-d-simulation",children:"Depth Camera and RGB-D Simulation"}),"\n",(0,a.jsx)(n.h3,{id:"depth-camera-fundamentals",children:"Depth Camera Fundamentals"}),"\n",(0,a.jsx)(n.p,{children:"Depth cameras in simulation must accurately model:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Pinhole camera model"}),": Geometric projection of 3D points to 2D image"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Depth measurement"}),": Distance from camera to scene points"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Noise characteristics"}),": Sensor-specific noise patterns"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Resolution limitations"}),": Finite pixel resolution and quantization"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"depth-camera-implementation",children:"Depth Camera Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'using UnityEngine;\nusing System.Collections;\n\npublic class DepthCameraSimulator : MonoBehaviour\n{\n    [Header("Camera Configuration")]\n    public int width = 640;\n    public int height = 480;\n    public float fov = 60f;\n    public float nearClip = 0.1f;\n    public float farClip = 10.0f;\n\n    [Header("Noise Parameters")]\n    public float depthNoiseStdDev = 0.01f;  // 1cm standard deviation\n    public float radialDistortion = 0.1f;   // Lens distortion\n    public float tangentialDistortion = 0.01f;\n\n    private Camera depthCamera;\n    private RenderTexture depthTexture;\n    private float[,] depthData;\n\n    void Start()\n    {\n        SetupDepthCamera();\n        CreateDepthTexture();\n        depthData = new float[width, height];\n    }\n\n    void SetupDepthCamera()\n    {\n        depthCamera = GetComponent<Camera>();\n        if (depthCamera == null)\n        {\n            depthCamera = gameObject.AddComponent<Camera>();\n        }\n\n        depthCamera.fieldOfView = fov;\n        depthCamera.nearClipPlane = nearClip;\n        depthCamera.farClipPlane = farClip;\n        depthCamera.depth = -1; // Render after other cameras\n        depthCamera.enabled = false; // Don\'t render automatically\n    }\n\n    void CreateDepthTexture()\n    {\n        depthTexture = new RenderTexture(width, height, 24, RenderTextureFormat.Depth);\n        depthTexture.Create();\n        depthCamera.targetTexture = depthTexture;\n    }\n\n    public float[,] GetDepthImage()\n    {\n        // Render the scene from depth camera\n        depthCamera.Render();\n\n        // Read depth data from texture\n        RenderTexture.active = depthTexture;\n        Texture2D depthTex = new Texture2D(width, height, TextureFormat.RGB24, false);\n        depthTex.ReadPixels(new Rect(0, 0, width, height), 0, 0);\n        depthTex.Apply();\n\n        // Convert texture to depth values\n        Color[] pixels = depthTex.GetPixels();\n\n        for (int y = 0; y < height; y++)\n        {\n            for (int x = 0; x < width; x++)\n            {\n                int pixelIndex = y * width + x;\n                Color pixel = pixels[pixelIndex];\n\n                // Convert color to depth value (simplified)\n                float rawDepth = pixel.r; // Assuming red channel contains depth info\n                float depth = ConvertRawDepthToMeters(rawDepth);\n\n                // Add noise to depth measurement\n                depth = AddDepthNoise(depth, x, y);\n\n                depthData[x, y] = depth;\n            }\n        }\n\n        // Clean up\n        RenderTexture.active = null;\n        DestroyImmediate(depthTex);\n\n        return depthData;\n    }\n\n    float ConvertRawDepthToMeters(float rawDepth)\n    {\n        // Convert normalized depth value to meters\n        // This depends on your specific depth rendering setup\n        float depth = nearClip + rawDepth * (farClip - nearClip);\n        return depth;\n    }\n\n    float AddDepthNoise(float depth, int x, int y)\n    {\n        // Add position-dependent noise\n        float noise = RandomGaussian() * depthNoiseStdDev;\n\n        // Add quantization noise based on pixel position\n        float quantization = (x % 4) * 0.001f + (y % 4) * 0.001f;\n\n        return depth + noise + quantization;\n    }\n\n    float RandomGaussian()\n    {\n        // Box-Muller transform\n        float u1 = Random.value;\n        float u2 = Random.value;\n\n        if (u1 < Mathf.Epsilon) u1 = Mathf.Epsilon;\n\n        return Mathf.Sqrt(-2.0f * Mathf.Log(u1)) * Mathf.Cos(2.0f * Mathf.PI * u2);\n    }\n\n    void OnDestroy()\n    {\n        if (depthTexture != null)\n        {\n            depthTexture.Release();\n        }\n    }\n}\n'})}),"\n",(0,a.jsx)(n.h3,{id:"rgb-d-sensor-pipeline",children:"RGB-D Sensor Pipeline"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'public class RGBDSensorPipeline : MonoBehaviour\n{\n    [Header("RGB-D Configuration")]\n    public DepthCameraSimulator depthCamera;\n    public Camera rgbCamera;\n    public int width = 640;\n    public int height = 480;\n\n    [Header("Calibration Parameters")]\n    public Vector2 principalPoint = new Vector2(320, 240);\n    public Vector2 focalLength = new Vector2(525, 525);\n    public float baseline = 0.075f; // For stereo cameras\n\n    private float[,] depthData;\n    private Color32[] rgbData;\n    private Vector3[,] pointCloud;\n\n    void Start()\n    {\n        InitializeRGBDPipeline();\n    }\n\n    void InitializeRGBDPipeline()\n    {\n        // Initialize depth camera\n        if (depthCamera == null)\n        {\n            depthCamera = GetComponent<DepthCameraSimulator>();\n        }\n\n        // Initialize RGB camera\n        if (rgbCamera == null)\n        {\n            rgbCamera = GetComponent<Camera>();\n        }\n\n        pointCloud = new Vector3[width, height];\n    }\n\n    public void UpdateSensorData()\n    {\n        // Get depth and RGB data\n        depthData = depthCamera.GetDepthImage();\n        rgbData = GetRGBData();\n\n        // Generate point cloud from depth data\n        GeneratePointCloud();\n    }\n\n    Color32[] GetRGBData()\n    {\n        // Capture RGB image from camera\n        RenderTexture currentRT = RenderTexture.active;\n        RenderTexture.active = rgbCamera.targetTexture;\n\n        Texture2D tex = new Texture2D(width, height, TextureFormat.RGB24, false);\n        tex.ReadPixels(new Rect(0, 0, width, height), 0, 0);\n        tex.Apply();\n\n        Color32[] pixels = tex.GetPixels32();\n        RenderTexture.active = currentRT;\n        DestroyImmediate(tex);\n\n        return pixels;\n    }\n\n    void GeneratePointCloud()\n    {\n        for (int y = 0; y < height; y++)\n        {\n            for (int x = 0; x < width; x++)\n            {\n                float depth = depthData[x, y];\n\n                if (depth > 0 && depth < depthCamera.farClip)\n                {\n                    // Convert pixel coordinates to 3D world coordinates\n                    float worldX = (x - principalPoint.x) * depth / focalLength.x;\n                    float worldY = (y - principalPoint.y) * depth / focalLength.y;\n                    float worldZ = depth;\n\n                    // Transform from camera to world coordinates\n                    Vector3 localPoint = new Vector3(worldX, worldY, worldZ);\n                    Vector3 worldPoint = transform.TransformPoint(localPoint);\n\n                    pointCloud[x, y] = worldPoint;\n                }\n                else\n                {\n                    pointCloud[x, y] = Vector3.zero; // Invalid point\n                }\n            }\n        }\n    }\n\n    public Vector3[,] GetPointCloud()\n    {\n        return pointCloud;\n    }\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"imu-modeling-and-drift",children:"IMU Modeling and Drift"}),"\n",(0,a.jsx)(n.h3,{id:"imu-fundamentals-in-simulation",children:"IMU Fundamentals in Simulation"}),"\n",(0,a.jsx)(n.p,{children:"An Inertial Measurement Unit (IMU) typically contains:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Accelerometer"}),": Measures linear acceleration"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Gyroscope"}),": Measures angular velocity"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Magnetometer"}),": Measures magnetic field (for heading)"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"IMU simulation must model various error sources:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Bias"}),": Constant offset in measurements"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Scale factor errors"}),": Proportional errors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Noise"}),": Random measurement variations"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Drift"}),": Time-dependent bias changes"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Temperature effects"}),": Performance variation with temperature"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"imu-simulation-implementation",children:"IMU Simulation Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'using UnityEngine;\nusing System;\n\npublic class IMUSimulator : MonoBehaviour\n{\n    [Header("IMU Configuration")]\n    public float accelerometerNoiseDensity = 0.002f;   // m/s^2/sqrt(Hz)\n    public float gyroscopeNoiseDensity = 0.0001f;      // rad/s/sqrt(Hz)\n    public float accelerometerRandomWalk = 0.001f;     // m/s^3/sqrt(Hz)\n    public float gyroscopeRandomWalk = 0.00001f;       // rad/s^2/sqrt(Hz)\n    public float magnetometerNoise = 0.1f;             // uT\n\n    [Header("Bias Parameters")]\n    public float accelerometerBiasStability = 0.01f;   // m/s^2\n    public float gyroscopeBiasStability = 0.001f;      // rad/s\n    public float biasCorrelationTime = 3600.0f;        // seconds\n\n    [Header("Temperature Effects")]\n    public float temperatureCoefficientAccel = 0.0001f; // (m/s^2)/degC\n    public float temperatureCoefficientGyro = 0.00001f; // (rad/s)/degC\n    public float ambientTemperature = 25.0f;           // degC\n\n    private Vector3 trueAcceleration;\n    private Vector3 trueAngularVelocity;\n    private Vector3 trueMagneticField;\n    private Vector3 accelerometerBias;\n    private Vector3 gyroscopeBias;\n    private Vector3 accelerometerWalk;\n    private Vector3 gyroscopeWalk;\n    private float lastUpdateTime;\n\n    void Start()\n    {\n        InitializeIMU();\n    }\n\n    void InitializeIMU()\n    {\n        // Initialize biases with random values within stability limits\n        accelerometerBias = new Vector3(\n            UnityEngine.Random.Range(-accelerometerBiasStability, accelerometerBiasStability),\n            UnityEngine.Random.Range(-accelerometerBiasStability, accelerometerBiasStability),\n            UnityEngine.Random.Range(-accelerometerBiasStability, accelerometerBiasStability)\n        );\n\n        gyroscopeBias = new Vector3(\n            UnityEngine.Random.Range(-gyroscopeBiasStability, gyroscopeBiasStability),\n            UnityEngine.Random.Range(-gyroscopeBiasStability, gyroscopeBiasStability),\n            UnityEngine.Random.Range(-gyroscopeBiasStability, gyroscopeBiasStability)\n        );\n\n        accelerometerWalk = Vector3.zero;\n        gyroscopeWalk = Vector3.zero;\n        lastUpdateTime = Time.time;\n    }\n\n    void Update()\n    {\n        UpdateIMUBias();\n    }\n\n    void UpdateIMUBias()\n    {\n        float deltaTime = Time.time - lastUpdateTime;\n        lastUpdateTime = Time.time;\n\n        // Update random walk components (first-order Gauss-Markov process)\n        float decayFactor = Mathf.Exp(-deltaTime / biasCorrelationTime);\n\n        accelerometerWalk += GetWhiteNoiseVector(accelerometerRandomWalk) * Mathf.Sqrt(deltaTime);\n        gyroscopeWalk += GetWhiteNoiseVector(gyroscopeRandomWalk) * Mathf.Sqrt(deltaTime);\n\n        // Apply decay to bias (bias instability over time)\n        accelerometerBias *= decayFactor;\n        gyroscopeBias *= decayFactor;\n\n        // Add new random walk components\n        accelerometerBias += accelerometerWalk * deltaTime;\n        gyroscopeBias += gyroscopeWalk * deltaTime;\n    }\n\n    public sensor_msgs.Imu GetIMUData()\n    {\n        // Get true values from the simulation\n        trueAcceleration = GetTrueAcceleration();\n        trueAngularVelocity = GetTrueAngularVelocity();\n        trueMagneticField = GetTrueMagneticField();\n\n        // Apply sensor model\n        Vector3 measuredAccel = ApplyAccelerometerModel(trueAcceleration);\n        Vector3 measuredGyro = ApplyGyroscopeModel(trueAngularVelocity);\n        Vector3 measuredMag = ApplyMagnetometerModel(trueMagneticField);\n\n        // Create ROS IMU message\n        sensor_msgs.Imu imuMsg = new sensor_msgs.Imu();\n\n        // Fill acceleration data\n        imuMsg.linear_acceleration = new geometry_msgs.Vector3(\n            measuredAccel.x, measuredAccel.y, measuredAccel.z\n        );\n\n        // Fill angular velocity data\n        imuMsg.angular_velocity = new geometry_msgs.Vector3(\n            measuredGyro.x, measuredGyro.y, measuredGyro.z\n        );\n\n        // Estimate orientation from gravity and magnetic field\n        imuMsg.orientation = EstimateOrientation(measuredAccel, measuredMag);\n\n        // Set header information\n        imuMsg.header.stamp = GetROSTime();\n        imuMsg.header.frame_id = "imu_link";\n\n        return imuMsg;\n    }\n\n    Vector3 ApplyAccelerometerModel(Vector3 trueAccel)\n    {\n        // Add temperature effect\n        float temperatureEffect = (ambientTemperature - 25.0f) * temperatureCoefficientAccel;\n        Vector3 tempEffectedAccel = trueAccel + Vector3.one * temperatureEffect;\n\n        // Add bias\n        Vector3 biasedAccel = tempEffectedAccel + accelerometerBias;\n\n        // Add noise\n        Vector3 noise = GetWhiteNoiseVector(accelerometerNoiseDensity);\n        Vector3 noisyAccel = biasedAccel + noise;\n\n        return noisyAccel;\n    }\n\n    Vector3 ApplyGyroscopeModel(Vector3 trueGyro)\n    {\n        // Add temperature effect\n        float temperatureEffect = (ambientTemperature - 25.0f) * temperatureCoefficientGyro;\n        Vector3 tempEffectedGyro = trueGyro + Vector3.one * temperatureEffect;\n\n        // Add bias\n        Vector3 biasedGyro = tempEffectedGyro + gyroscopeBias;\n\n        // Add noise\n        Vector3 noise = GetWhiteNoiseVector(gyroscopeNoiseDensity);\n        Vector3 noisyGyro = biasedGyro + noise;\n\n        return noisyGyro;\n    }\n\n    Vector3 ApplyMagnetometerModel(Vector3 trueMag)\n    {\n        // Add noise to magnetic field measurement\n        Vector3 noise = GetWhiteNoiseVector(magnetometerNoise);\n        return trueMag + noise;\n    }\n\n    Vector3 GetTrueAcceleration()\n    {\n        // Get true acceleration from physics simulation\n        Rigidbody rb = GetComponent<Rigidbody>();\n        if (rb != null)\n        {\n            // True acceleration is the derivative of velocity plus gravity\n            Vector3 gravity = Physics.gravity;\n            return rb.velocity / Time.fixedDeltaTime + gravity;\n        }\n\n        // Fallback: assume zero acceleration if no rigidbody\n        return Vector3.zero;\n    }\n\n    Vector3 GetTrueAngularVelocity()\n    {\n        // Get true angular velocity from physics simulation\n        Rigidbody rb = GetComponent<Rigidbody>();\n        if (rb != null)\n        {\n            return rb.angularVelocity;\n        }\n\n        // Fallback: assume zero angular velocity\n        return Vector3.zero;\n    }\n\n    Vector3 GetTrueMagneticField()\n    {\n        // Return Earth\'s magnetic field (simplified)\n        // In real implementation, this would account for local magnetic anomalies\n        return new Vector3(0.22f, 0.0f, 0.45f); // ~45 degree inclination, 0.5 uT magnitude\n    }\n\n    Vector3 GetWhiteNoiseVector(float noiseDensity)\n    {\n        return new Vector3(\n            RandomGaussian() * noiseDensity,\n            RandomGaussian() * noiseDensity,\n            RandomGaussian() * noiseDensity\n        );\n    }\n\n    geometry_msgs.Quaternion EstimateOrientation(Vector3 accel, Vector3 mag)\n    {\n        // Simple orientation estimation from accelerometer and magnetometer\n        // This is a simplified version - real implementation would use more sophisticated filtering\n\n        // Normalize vectors\n        Vector3 normalizedAccel = accel.normalized;\n        Vector3 normalizedMag = mag.normalized;\n\n        // Create coordinate system\n        Vector3 zAxis = -normalizedAccel; // Accelerometer points opposite to gravity\n        Vector3 xAxis = Vector3.Cross(normalizedMag, zAxis).normalized;\n        Vector3 yAxis = Vector3.Cross(zAxis, xAxis);\n\n        // Create rotation matrix and convert to quaternion\n        Matrix4x4 rotationMatrix = new Matrix4x4();\n        rotationMatrix.SetColumn(0, new Vector4(xAxis.x, yAxis.x, zAxis.x, 0));\n        rotationMatrix.SetColumn(1, new Vector4(xAxis.y, yAxis.y, zAxis.y, 0));\n        rotationMatrix.SetColumn(2, new Vector4(xAxis.z, yAxis.z, zAxis.z, 0));\n        rotationMatrix.SetColumn(3, new Vector4(0, 0, 0, 1));\n\n        Quaternion orientation = Quaternion.LookRotation(zAxis, yAxis);\n\n        return new geometry_msgs.Quaternion(\n            orientation.x, orientation.y, orientation.z, orientation.w\n        );\n    }\n\n    builtin_interfaces.Time GetROSTime()\n    {\n        // Return current ROS time\n        double rosTime = Time.time;\n        builtin_interfaces.Time time = new builtin_interfaces.Time();\n        time.sec = (int)rosTime;\n        time.nanosec = (uint)((rosTime - time.sec) * 1e9);\n        return time;\n    }\n\n    float RandomGaussian()\n    {\n        float u1 = UnityEngine.Random.value;\n        float u2 = UnityEngine.Random.value;\n\n        if (u1 < Mathf.Epsilon) u1 = Mathf.Epsilon;\n\n        return Mathf.Sqrt(-2.0f * Mathf.Log(u1)) * Mathf.Cos(2.0f * Mathf.PI * u2);\n    }\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"sensor-synchronization-and-data-accuracy",children:"Sensor Synchronization and Data Accuracy"}),"\n",(0,a.jsx)(n.h3,{id:"time-synchronization-challenges",children:"Time Synchronization Challenges"}),"\n",(0,a.jsx)(n.p,{children:"In multi-sensor systems, synchronization is critical for accurate perception. Key challenges include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Clock drift"}),": Different sensors may have slightly different time bases"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Latency variations"}),": Processing delays can vary between sensors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Update rates"}),": Different sensors may operate at different frequencies"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"synchronization-implementation",children:"Synchronization Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'using System.Collections.Generic;\n\npublic class SensorSynchronizer : MonoBehaviour\n{\n    [Header("Synchronization Parameters")]\n    public float maxTimeDifference = 0.01f; // 10ms tolerance\n    public int maxBufferSize = 100;         // Maximum messages to buffer\n\n    private Dictionary<string, Queue<SensorMessage>> sensorBuffers;\n    private List<SynchronizedData> synchronizedData;\n    private float lastSynchronizationTime;\n\n    [System.Serializable]\n    public class SensorMessage\n    {\n        public string sensorType;\n        public double timestamp;\n        public object data;\n    }\n\n    [System.Serializable]\n    public class SynchronizedData\n    {\n        public double timestamp;\n        public Dictionary<string, object> sensorData;\n    }\n\n    void Start()\n    {\n        InitializeSynchronization();\n    }\n\n    void InitializeSynchronization()\n    {\n        sensorBuffers = new Dictionary<string, Queue<SensorMessage>>();\n        synchronizedData = new List<SynchronizedData>();\n        lastSynchronizationTime = Time.time;\n    }\n\n    public void AddSensorData(string sensorType, double timestamp, object data)\n    {\n        if (!sensorBuffers.ContainsKey(sensorType))\n        {\n            sensorBuffers[sensorType] = new Queue<SensorMessage>();\n        }\n\n        SensorMessage message = new SensorMessage\n        {\n            sensorType = sensorType,\n            timestamp = timestamp,\n            data = data\n        };\n\n        sensorBuffers[sensorType].Enqueue(message);\n\n        // Limit buffer size\n        if (sensorBuffers[sensorType].Count > maxBufferSize)\n        {\n            sensorBuffers[sensorType].Dequeue();\n        }\n\n        // Attempt synchronization\n        AttemptSynchronization();\n    }\n\n    void AttemptSynchronization()\n    {\n        // Check if we have data from all sensors\n        if (AllSensorsHaveData())\n        {\n            // Find the closest timestamps across all sensors\n            double referenceTime = FindReferenceTimestamp();\n\n            // Check if timestamps are within tolerance\n            if (AreTimestampsWithinTolerance(referenceTime))\n            {\n                // Create synchronized data package\n                SynchronizedData syncData = CreateSynchronizedData(referenceTime);\n\n                if (syncData != null)\n                {\n                    synchronizedData.Add(syncData);\n\n                    // Remove synchronized messages from buffers\n                    RemoveSynchronizedMessages(referenceTime);\n                }\n            }\n        }\n    }\n\n    bool AllSensorsHaveData()\n    {\n        // Check if all registered sensors have at least one message\n        foreach (var buffer in sensorBuffers.Values)\n        {\n            if (buffer.Count == 0)\n                return false;\n        }\n        return sensorBuffers.Count > 0;\n    }\n\n    double FindReferenceTimestamp()\n    {\n        // Find the median timestamp across all sensors\n        List<double> allTimestamps = new List<double>();\n\n        foreach (var buffer in sensorBuffers.Values)\n        {\n            if (buffer.Count > 0)\n            {\n                allTimestamps.Add(buffer.Peek().timestamp);\n            }\n        }\n\n        allTimestamps.Sort();\n\n        if (allTimestamps.Count == 0)\n            return 0;\n\n        int middleIndex = allTimestamps.Count / 2;\n        return allTimestamps[middleIndex];\n    }\n\n    bool AreTimestampsWithinTolerance(double referenceTime)\n    {\n        foreach (var buffer in sensorBuffers.Values)\n        {\n            if (buffer.Count > 0)\n            {\n                double timeDiff = Mathf.Abs((float)(buffer.Peek().timestamp - referenceTime));\n                if (timeDiff > maxTimeDifference)\n                    return false;\n            }\n        }\n        return true;\n    }\n\n    SynchronizedData CreateSynchronizedData(double referenceTime)\n    {\n        SynchronizedData syncData = new SynchronizedData();\n        syncData.timestamp = referenceTime;\n        syncData.sensorData = new Dictionary<string, object>();\n\n        foreach (var kvp in sensorBuffers)\n        {\n            string sensorType = kvp.Key;\n            Queue<SensorMessage> buffer = kvp.Value;\n\n            if (buffer.Count > 0)\n            {\n                SensorMessage message = buffer.Peek();\n                syncData.sensorData[sensorType] = message.data;\n            }\n        }\n\n        return syncData;\n    }\n\n    void RemoveSynchronizedMessages(double referenceTime)\n    {\n        foreach (var kvp in sensorBuffers)\n        {\n            Queue<SensorMessage> buffer = kvp.Value;\n\n            if (buffer.Count > 0)\n            {\n                SensorMessage message = buffer.Peek();\n                double timeDiff = Mathf.Abs((float)(message.timestamp - referenceTime));\n\n                if (timeDiff <= maxTimeDifference)\n                {\n                    buffer.Dequeue();\n                }\n            }\n        }\n    }\n\n    public List<SynchronizedData> GetSynchronizedData()\n    {\n        return synchronizedData;\n    }\n\n    public void ClearSynchronizedData()\n    {\n        synchronizedData.Clear();\n    }\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"evaluating-sensor-realism-and-limitations",children:"Evaluating Sensor Realism and Limitations"}),"\n",(0,a.jsx)(n.h3,{id:"sensor-fidelity-metrics",children:"Sensor Fidelity Metrics"}),"\n",(0,a.jsx)(n.p,{children:"To evaluate the realism of sensor simulation, consider these metrics:"}),"\n",(0,a.jsx)(n.h4,{id:"accuracy-metrics",children:"Accuracy Metrics"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Absolute error"}),": Difference between simulated and real measurements"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Precision"}),": Consistency of repeated measurements"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Bias"}),": Systematic offset in measurements"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Drift"}),": Time-dependent changes in bias"]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Update rate"}),": Frequency of sensor data generation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Latency"}),": Delay between physical event and sensor reading"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Throughput"}),": Data processing capacity"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"validation-against-real-hardware",children:"Validation Against Real Hardware"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'public class SensorValidator : MonoBehaviour\n{\n    [Header("Validation Parameters")]\n    public TextAsset realSensorData;  // CSV file with real sensor data\n    public float validationThreshold = 0.1f;  // Acceptable error threshold\n\n    private List<SensorReading> realData;\n    private List<SensorReading> simulatedData;\n\n    [System.Serializable]\n    public class SensorReading\n    {\n        public double timestamp;\n        public Vector3 data;  // For 3-axis sensors like IMU\n        public float confidence;\n    }\n\n    void Start()\n    {\n        LoadRealSensorData();\n    }\n\n    void LoadRealSensorData()\n    {\n        if (realSensorData != null)\n        {\n            string[] lines = realSensorData.text.Split(\'\\n\');\n            realData = new List<SensorReading>();\n\n            foreach (string line in lines)\n            {\n                string[] values = line.Split(\',\');\n                if (values.Length >= 4)  // timestamp, x, y, z\n                {\n                    SensorReading reading = new SensorReading\n                    {\n                        timestamp = double.Parse(values[0]),\n                        data = new Vector3(\n                            float.Parse(values[1]),\n                            float.Parse(values[2]),\n                            float.Parse(values[3])\n                        )\n                    };\n                    realData.Add(reading);\n                }\n            }\n        }\n    }\n\n    public float CalculateFidelityMetric()\n    {\n        if (realData == null || simulatedData == null ||\n            realData.Count != simulatedData.Count)\n        {\n            return 0.0f;  // Cannot calculate with mismatched data\n        }\n\n        float totalError = 0.0f;\n        int validComparisons = 0;\n\n        for (int i = 0; i < Mathf.Min(realData.Count, simulatedData.Count); i++)\n        {\n            float error = Vector3.Distance(realData[i].data, simulatedData[i].data);\n            totalError += error;\n            validComparisons++;\n        }\n\n        if (validComparisons > 0)\n        {\n            float averageError = totalError / validComparisons;\n            // Convert to fidelity score (0-1, where 1 is perfect)\n            return Mathf.Clamp01(1.0f - (averageError / validationThreshold));\n        }\n\n        return 0.0f;\n    }\n\n    public void CompareWithRealData()\n    {\n        float fidelity = CalculateFidelityMetric();\n        Debug.Log($"Sensor fidelity: {fidelity * 100:F2}%");\n\n        if (fidelity < 0.8f)\n        {\n            Debug.LogWarning("Sensor simulation fidelity is below 80%. Consider adjusting noise parameters.");\n        }\n    }\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices-for-sensor-simulation",children:"Best Practices for Sensor Simulation"}),"\n",(0,a.jsx)(n.h3,{id:"modeling-guidelines",children:"Modeling Guidelines"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Characterize Real Sensors"}),": Measure actual sensor noise, bias, and drift parameters"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Validate Against Hardware"}),": Compare simulation output with real sensor data"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Consider Environmental Factors"}),": Include temperature, humidity, and lighting effects"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Account for Sensor Placement"}),": Model mounting position and orientation errors"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Efficient Raycasting"}),": Use optimized algorithms for LiDAR simulation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Texture Compression"}),": Balance image quality with performance for RGB-D sensors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Update Rate Management"}),": Match simulation update rates to real sensor frequencies"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Memory Management"}),": Efficiently handle large point cloud and image data"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"integration-strategies",children:"Integration Strategies"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Modular Design"}),": Create separate components for each sensor type"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Calibration Support"}),": Include parameters for sensor calibration"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"ROS Message Compatibility"}),": Ensure proper message format for robotics frameworks"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Logging"}),": Include facilities for recording and analyzing sensor data"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"This chapter covered the implementation of realistic sensor simulation for digital twins in humanoid robotics. We explored LiDAR simulation with proper noise modeling, depth camera and RGB-D pipeline implementation, IMU modeling with drift characteristics, and sensor synchronization techniques. The fidelity of sensor simulation is crucial for the transferability of algorithms from simulation to reality, and proper validation against real hardware ensures that the digital twin accurately represents the physical system."})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);